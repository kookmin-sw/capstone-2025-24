{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ihj09xyyb0C2","executionInfo":{"status":"ok","timestamp":1742534834446,"user_tz":-540,"elapsed":27756,"user":{"displayName":"ê¹€ì˜ˆí–¥","userId":"01564514346289315086"}},"outputId":"c6d208a7-53b8-48ba-9123-2880a77de7fa"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OQnBdpM6a5n-","executionInfo":{"status":"ok","timestamp":1742010143639,"user_tz":-540,"elapsed":5,"user":{"displayName":"ê¹€ì˜ˆí–¥","userId":"01564514346289315086"}},"outputId":"566c3afe-fd63-42ee-f180-ff7df29f0652"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/YOLO\n"]}],"source":["%cd drive/MyDrive/YOLO"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"7ZqSsM17a5n_","executionInfo":{"status":"ok","timestamp":1742007681595,"user_tz":-540,"elapsed":2808,"user":{"displayName":"ê¹€ì˜ˆí–¥","userId":"01564514346289315086"}},"outputId":"6862be39-cf04-4059-90df-2730a8f6170c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: ultralytics in /usr/local/lib/python3.11/dist-packages (8.3.90)\n","Requirement already satisfied: numpy<=2.1.1,>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.26.4)\n","Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (3.10.0)\n","Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.11.0.86)\n","Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (11.1.0)\n","Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.2)\n","Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.3)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.14.1)\n","Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.6.0+cu124)\n","Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.21.0+cu124)\n","Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.67.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (5.9.5)\n","Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\n","Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.2.2)\n","Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.13.2)\n","Requirement already satisfied: ultralytics-thop>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.0.14)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.56.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2025.1.31)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.17.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2024.10.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n"]}],"source":["!pip install ultralytics"]},{"cell_type":"markdown","source":["### ë°ì´í„° ì¤€ë¹„"],"metadata":{"id":"cXMnglCAq9hp"}},{"cell_type":"code","source":["# !pip install roboflow"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"EE9f6Qpfcyg6","executionInfo":{"status":"ok","timestamp":1741935634003,"user_tz":-540,"elapsed":2925,"user":{"displayName":"ê¹€ì˜ˆí–¥","userId":"01564514346289315086"}},"outputId":"96a5a449-ffdc-4c85-acb2-e3382f4b167e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: roboflow in /usr/local/lib/python3.11/dist-packages (1.1.58)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from roboflow) (2025.1.31)\n","Requirement already satisfied: idna==3.7 in /usr/local/lib/python3.11/dist-packages (from roboflow) (3.7)\n","Requirement already satisfied: cycler in /usr/local/lib/python3.11/dist-packages (from roboflow) (0.12.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.4.8)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from roboflow) (3.10.0)\n","Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.26.4)\n","Requirement already satisfied: opencv-python-headless==4.10.0.84 in /usr/local/lib/python3.11/dist-packages (from roboflow) (4.10.0.84)\n","Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from roboflow) (11.1.0)\n","Requirement already satisfied: pillow-heif>=0.18.0 in /usr/local/lib/python3.11/dist-packages (from roboflow) (0.21.0)\n","Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.8.2)\n","Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.0.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.32.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.17.0)\n","Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.3.0)\n","Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from roboflow) (4.67.1)\n","Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from roboflow) (6.0.2)\n","Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.0.0)\n","Requirement already satisfied: filetype in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.2.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (1.3.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (4.56.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (24.2)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (3.2.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->roboflow) (3.4.1)\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"kGDEwT_Oa5oA","executionInfo":{"status":"ok","timestamp":1741967932392,"user_tz":-540,"elapsed":92060,"user":{"displayName":"ê¹€ì˜ˆí–¥","userId":"01564514346289315086"}},"outputId":"a54c0593-fbd0-4353-94a1-50e9e9ee5732"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting roboflow\n","  Downloading roboflow-1.1.58-py3-none-any.whl.metadata (9.7 kB)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from roboflow) (2025.1.31)\n","Collecting idna==3.7 (from roboflow)\n","  Downloading idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n","Requirement already satisfied: cycler in /usr/local/lib/python3.11/dist-packages (from roboflow) (0.12.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.4.8)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from roboflow) (3.10.0)\n","Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.26.4)\n","Collecting opencv-python-headless==4.10.0.84 (from roboflow)\n","  Downloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n","Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from roboflow) (11.1.0)\n","Collecting pillow-heif>=0.18.0 (from roboflow)\n","  Downloading pillow_heif-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.8 kB)\n","Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.8.2)\n","Collecting python-dotenv (from roboflow)\n","  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.32.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.17.0)\n","Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.3.0)\n","Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from roboflow) (4.67.1)\n","Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from roboflow) (6.0.2)\n","Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.0.0)\n","Collecting filetype (from roboflow)\n","  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (1.3.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (4.56.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (24.2)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (3.2.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->roboflow) (3.4.1)\n","Downloading roboflow-1.1.58-py3-none-any.whl (84 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.5/84.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading idna-3.7-py3-none-any.whl (66 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.9 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m49.9/49.9 MB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pillow_heif-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m99.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n","Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n","Installing collected packages: filetype, python-dotenv, pillow-heif, opencv-python-headless, idna, roboflow\n","  Attempting uninstall: opencv-python-headless\n","    Found existing installation: opencv-python-headless 4.11.0.86\n","    Uninstalling opencv-python-headless-4.11.0.86:\n","      Successfully uninstalled opencv-python-headless-4.11.0.86\n","  Attempting uninstall: idna\n","    Found existing installation: idna 3.10\n","    Uninstalling idna-3.10:\n","      Successfully uninstalled idna-3.10\n","Successfully installed filetype-1.2.0 idna-3.7 opencv-python-headless-4.10.0.84 pillow-heif-0.21.0 python-dotenv-1.0.1 roboflow-1.1.58\n","loading Roboflow workspace...\n","loading Roboflow project...\n"]},{"output_type":"stream","name":"stderr","text":["Downloading Dataset Version Zip in detection-finetuing-data-1 to yolov11:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 146397/146397 [00:15<00:00, 9748.94it/s]"]},{"output_type":"stream","name":"stdout","text":["\n"]},{"output_type":"stream","name":"stderr","text":["\n","Extracting Dataset Version Zip to detection-finetuing-data-1 in yolov11:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6032/6032 [01:04<00:00, 92.99it/s]\n"]}],"source":["# !pip install roboflow\n","\n","# from roboflow import Roboflow\n","# rf = Roboflow(api_key=\"4ttkOuqunWioZ3nCy8dy\")\n","# project = rf.workspace(\"justpers\").project(\"detection-finetuing-data\")\n","# version = project.version(1)\n","# dataset = version.download(\"yolov11\")"]},{"cell_type":"code","source":["!pip install wandb\n","!wandb login ad541938b9cdba0c8991d9905ec5456ed72acbb0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"24HAC80ccxYx","executionInfo":{"status":"ok","timestamp":1741969848113,"user_tz":-540,"elapsed":8755,"user":{"displayName":"ê¹€ì˜ˆí–¥","userId":"01564514346289315086"}},"outputId":"e01560e0-3619-458f-e232-070b813878bf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.8)\n","Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\n","Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n","Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n","Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.6)\n","Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.25.6)\n","Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n","Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.10.6)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n","Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n","Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.22.0)\n","Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.5)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.1.0)\n","Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.12.2)\n","Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (2.27.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n","\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]}]},{"cell_type":"code","source":["%cd yolov11n-finetuning"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DVIcIZNiRJXV","executionInfo":{"status":"ok","timestamp":1742010147671,"user_tz":-540,"elapsed":6,"user":{"displayName":"ê¹€ì˜ˆí–¥","userId":"01564514346289315086"}},"outputId":"b502c41b-71d5-47e4-e42d-354fefea9d0c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/YOLO/yolov11n-finetuning\n"]}]},{"cell_type":"markdown","source":["### ëª¨ë¸ í•™ìŠµ"],"metadata":{"id":"glTwubBbloXX"}},{"cell_type":"code","source":["from Detectify import Train\n","\n","train = Train()\n","\n","train.start(dataset=\"/content/drive/MyDrive/YOLO/yolov11n-finetuning/data.yaml\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"qxIQeH6sKeA8","executionInfo":{"status":"error","timestamp":1741976054465,"user_tz":-540,"elapsed":6083875,"user":{"displayName":"ê¹€ì˜ˆí–¥","userId":"01564514346289315086"}},"outputId":"beff8800-cc7a-4eca-b9df-a9343329c9ce"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mì•Œë¦¼\u001b[0m\u001b[1m: \u001b[0m'yolo11n.pt'ì„ ë¶ˆëŸ¬ì˜¤ê³  ìˆìŠµë‹ˆë‹¤.\n","\u001b[32m\u001b[1mì„±ê³µ\u001b[0m\u001b[1m: \u001b[0m'yolo11n.pt'ë¥¼ ì„±ê³µì ìœ¼ë¡œ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.\n","\u001b[34m\u001b[1mì•Œë¦¼\u001b[0m\u001b[1m: \u001b[0më°ì´í„°ì…‹ ìœ íš¨ì„± ê²€ì‚¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.\n","\u001b[32m\u001b[1mì„±ê³µ\u001b[0m\u001b[1m: \u001b[0m/content/drive/MyDrive/YOLO/yolov11n-finetuning/data.yaml ë‚´ì˜ train ë°ì´í„°ì…‹ ìœ íš¨ì„± ê²€ì‚¬ë¥¼ í†µê³¼í–ˆìŠµë‹ˆë‹¤.\n","\u001b[32m\u001b[1mì„±ê³µ\u001b[0m\u001b[1m: \u001b[0m/content/drive/MyDrive/YOLO/yolov11n-finetuning/data.yaml ë‚´ì˜ val ë°ì´í„°ì…‹ ìœ íš¨ì„± ê²€ì‚¬ë¥¼ í†µê³¼í–ˆìŠµë‹ˆë‹¤.\n","\u001b[32m\u001b[1mì„±ê³µ\u001b[0m\u001b[1m: \u001b[0m/content/drive/MyDrive/YOLO/yolov11n-finetuning/data.yaml ë‚´ì˜ test ë°ì´í„°ì…‹ ìœ íš¨ì„± ê²€ì‚¬ë¥¼ í†µê³¼í–ˆìŠµë‹ˆë‹¤.\n","\u001b[34m\u001b[1mì•Œë¦¼\u001b[0m\u001b[1m: \u001b[0mGPU ê°€ì† ì—¬ë¶€ë¥¼ í™•ì¸í•˜ê³  ìˆìŠµë‹ˆë‹¤.\n","\u001b[32m\u001b[1mì„±ê³µ\u001b[0m\u001b[1m: \u001b[0mGPU ê°€ì†ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnunajiyo\u001b[0m (\u001b[33mnunajiyo-kookmin-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.19.8"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/drive/MyDrive/YOLO/yolov11n-finetuning/wandb/run-20250314_163252-p00byoqz</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/nunajiyo-kookmin-university/yolov11-finetuning/runs/p00byoqz' target=\"_blank\">exp_yolo11n</a></strong> to <a href='https://wandb.ai/nunajiyo-kookmin-university/yolov11-finetuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/nunajiyo-kookmin-university/yolov11-finetuning' target=\"_blank\">https://wandb.ai/nunajiyo-kookmin-university/yolov11-finetuning</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/nunajiyo-kookmin-university/yolov11-finetuning/runs/p00byoqz' target=\"_blank\">https://wandb.ai/nunajiyo-kookmin-university/yolov11-finetuning/runs/p00byoqz</a>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mì•Œë¦¼\u001b[0m\u001b[1m: \u001b[0míŒŒì¸íŠœë‹ì„ ì‹œì‘í•©ë‹ˆë‹¤.\n","Ultralytics 8.3.90 ğŸš€ Python-3.11.11 torch-2.5.1+cu124 CUDA:0 (Tesla T4, 15095MiB)\n","\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolo11n.pt, data=/content/drive/MyDrive/YOLO/yolov11n-finetuning/data.yaml, epochs=50, time=None, patience=0, batch=16, imgsz=640, save=True, save_period=10, cache=False, device=[0], workers=0, project=output, name=train, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=output/train\n","Downloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n"]},{"output_type":"stream","name":"stderr","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 755k/755k [00:00<00:00, 124MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Overriding model.yaml nc=80 with nc=2\n","\n","                   from  n    params  module                                       arguments                     \n","  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n","  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n","  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n","  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n","  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n","  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n","  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n","  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n","  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n","  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n"," 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n"," 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n"," 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n"," 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n"," 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n"," 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n"," 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n"," 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n"," 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n"," 23        [16, 19, 22]  1    431062  ultralytics.nn.modules.head.Detect           [2, [64, 128, 256]]           \n","YOLO11n summary: 181 layers, 2,590,230 parameters, 2,590,214 gradients, 6.4 GFLOPs\n","\n","Transferred 448/499 items from pretrained weights\n","\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir output/train', view at http://localhost:6006/\n","Freezing layer 'model.23.dfl.conv.weight'\n","\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/YOLO/yolov11n-finetuning/detection-finetuing-data-1/train/labels... 2002 images, 4 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2002/2002 [28:15<00:00,  1.18it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/drive/MyDrive/YOLO/yolov11n-finetuning/detection-finetuing-data-1/train/labels.cache\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/YOLO/yolov11n-finetuning/detection-finetuing-data-1/valid/labels... 607 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 607/607 [08:42<00:00,  1.16it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/drive/MyDrive/YOLO/yolov11n-finetuning/detection-finetuing-data-1/valid/labels.cache\n","Plotting labels to output/train/labels.jpg... \n","\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n","\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001667, momentum=0.9) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\n","\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\n","Image sizes 640 train, 640 val\n","Using 0 dataloader workers\n","Logging results to \u001b[1moutput/train\u001b[0m\n","Starting training for 50 epochs...\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["       1/50      2.24G      1.565      2.742      1.468          3        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 126/126 [01:08<00:00,  1.83it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:12<00:00,  1.51it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all        607       1032      0.428       0.37      0.336      0.188\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["       2/50      2.63G      1.666      2.143      1.541          2        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 126/126 [01:05<00:00,  1.92it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:10<00:00,  1.78it/s]\n"]},{"output_type":"stream","name":"stdout","text":["                   all        607       1032      0.456      0.325       0.27      0.137\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["       3/50      2.63G      1.749      2.007      1.628          3        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 126/126 [01:03<00:00,  1.97it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:10<00:00,  1.76it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all        607       1032      0.546      0.353      0.365      0.188\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["       4/50      2.63G      1.731      1.826       1.61          5        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 126/126 [01:04<00:00,  1.94it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:10<00:00,  1.81it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all        607       1032      0.576      0.419      0.435      0.224\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["       5/50      2.63G      1.681      1.718      1.578          2        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 126/126 [01:16<00:00,  1.66it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:10<00:00,  1.88it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all        607       1032      0.571       0.42       0.41      0.222\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["       6/50      2.63G      1.641      1.598      1.525          4        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 126/126 [01:04<00:00,  1.96it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:10<00:00,  1.75it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all        607       1032      0.574      0.462      0.474      0.253\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["       7/50      2.63G      1.633      1.517       1.51          5        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 126/126 [01:14<00:00,  1.70it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:11<00:00,  1.70it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all        607       1032      0.639      0.429      0.442      0.256\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["       8/50      2.63G      1.583      1.473      1.502          5        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 126/126 [01:13<00:00,  1.71it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:11<00:00,  1.70it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all        607       1032      0.671      0.511      0.529      0.296\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["       9/50      2.63G       1.55      1.433       1.47          3        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 126/126 [01:07<00:00,  1.88it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:10<00:00,  1.73it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all        607       1032      0.664      0.468       0.53      0.314\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["      10/50      2.63G      1.541      1.406      1.463          3        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 126/126 [01:03<00:00,  1.99it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:10<00:00,  1.75it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all        607       1032      0.711      0.513      0.577      0.327\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["      11/50      2.63G      1.503      1.317      1.443          3        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 126/126 [01:04<00:00,  1.94it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:10<00:00,  1.75it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all        607       1032      0.652      0.544      0.601      0.344\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["      12/50      2.63G      1.499      1.339      1.441          3        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 126/126 [01:04<00:00,  1.95it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:10<00:00,  1.75it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all        607       1032      0.728      0.525      0.573       0.34\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["      13/50      2.63G      1.475      1.284      1.417          2        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 126/126 [01:04<00:00,  1.96it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:10<00:00,  1.78it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all        607       1032      0.668      0.545      0.576      0.339\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["      14/50      2.63G      1.436       1.28        1.4          6        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 126/126 [01:02<00:00,  2.00it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:10<00:00,  1.76it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all        607       1032       0.76      0.554      0.636      0.382\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["      15/50      2.63G      1.448      1.244      1.406          4        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 126/126 [01:04<00:00,  1.96it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:10<00:00,  1.85it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all        607       1032      0.734      0.531       0.61      0.373\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["      16/50      2.63G      1.442      1.266      1.405         17        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 126/126 [01:04<00:00,  1.95it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:10<00:00,  1.74it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all        607       1032      0.698      0.567      0.621       0.38\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["      17/50      2.63G      1.406      1.202       1.37         12        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 126/126 [01:04<00:00,  1.96it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:10<00:00,  1.82it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all        607       1032       0.73      0.573      0.643      0.388\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["      18/50      2.63G      1.364      1.147      1.354          3        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 126/126 [01:05<00:00,  1.93it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:10<00:00,  1.73it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all        607       1032      0.734      0.592      0.657      0.393\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["      19/50      2.63G      1.365       1.17      1.354          4        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 126/126 [01:04<00:00,  1.95it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:10<00:00,  1.78it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all        607       1032      0.791      0.532      0.617      0.376\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["      20/50      2.63G      1.372      1.122      1.333          6        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 126/126 [01:04<00:00,  1.95it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:11<00:00,  1.72it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all        607       1032       0.71      0.612      0.657      0.403\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["      21/50      2.63G      1.369      1.123      1.343          7        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 126/126 [01:03<00:00,  1.97it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:10<00:00,  1.76it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all        607       1032      0.763      0.556      0.633      0.381\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["      22/50      2.63G      1.335      1.105      1.327          7        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 126/126 [01:04<00:00,  1.94it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:10<00:00,  1.75it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all        607       1032       0.78      0.554      0.654        0.4\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["      23/50      2.63G      1.324      1.086       1.32          8        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 126/126 [01:04<00:00,  1.95it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:11<00:00,  1.68it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all        607       1032      0.787      0.597      0.667      0.416\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["      24/50      2.63G      1.322      1.062      1.326          2        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 126/126 [01:06<00:00,  1.90it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:11<00:00,  1.72it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all        607       1032      0.766      0.566      0.654      0.401\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["      25/50      2.63G      1.327      1.075      1.329          4        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 126/126 [01:04<00:00,  1.96it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:10<00:00,  1.77it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all        607       1032      0.805      0.599      0.692      0.426\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["      26/50      2.63G      1.301      1.028      1.301          5        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 126/126 [01:04<00:00,  1.95it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:10<00:00,  1.76it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all        607       1032      0.778      0.607      0.675      0.425\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["      27/50      2.63G      1.269      1.019      1.284          7        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 126/126 [01:03<00:00,  1.98it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:10<00:00,  1.76it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all        607       1032      0.842      0.594      0.702      0.445\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["      28/50      2.63G       1.28      1.002      1.299          6        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 126/126 [01:04<00:00,  1.96it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:10<00:00,  1.82it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all        607       1032      0.816      0.606      0.679      0.434\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["      29/50      2.63G      1.276     0.9922      1.296          6        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 126/126 [01:03<00:00,  1.97it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:10<00:00,  1.75it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all        607       1032      0.781      0.605      0.698      0.444\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["      30/50      2.63G      1.255          1      1.285          6        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 126/126 [01:04<00:00,  1.96it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:09<00:00,  1.90it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all        607       1032      0.826      0.614      0.709      0.441\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["      31/50      2.63G       1.24     0.9692      1.272          5        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 126/126 [01:05<00:00,  1.92it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:10<00:00,  1.75it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all        607       1032       0.79      0.613      0.688      0.444\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["      32/50      2.63G      1.234     0.9661      1.268          5        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 126/126 [01:03<00:00,  1.97it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:10<00:00,  1.80it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all        607       1032      0.804      0.624      0.701      0.451\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["      33/50      2.63G      1.222     0.9408      1.264          6        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 126/126 [01:04<00:00,  1.94it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:10<00:00,  1.73it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all        607       1032      0.818      0.626      0.718      0.466\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["      34/50      2.63G      1.223     0.9177      1.262          2        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 126/126 [01:04<00:00,  1.95it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:10<00:00,  1.79it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all        607       1032        0.8      0.641      0.717      0.462\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["      35/50      2.63G      1.225     0.9377      1.262          6        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 126/126 [01:04<00:00,  1.95it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:10<00:00,  1.73it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all        607       1032      0.833      0.631      0.708      0.454\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["      36/50      2.63G      1.205     0.9246      1.254          2        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 126/126 [01:05<00:00,  1.92it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:10<00:00,  1.75it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all        607       1032      0.785      0.631      0.722      0.461\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["      37/50      2.63G      1.194     0.9089      1.246          7        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 126/126 [01:06<00:00,  1.90it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:11<00:00,  1.72it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all        607       1032       0.83      0.624      0.729      0.466\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["      38/50      2.63G      1.192      0.888      1.246          6        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 126/126 [01:05<00:00,  1.92it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:10<00:00,  1.82it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all        607       1032      0.883      0.617      0.732      0.473\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["      39/50      2.63G      1.157     0.8568      1.213          7        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 126/126 [01:05<00:00,  1.94it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:10<00:00,  1.82it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all        607       1032      0.831      0.633      0.731      0.473\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["      40/50      2.63G      1.184     0.9011      1.239          8        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 126/126 [01:04<00:00,  1.95it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:10<00:00,  1.81it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all        607       1032      0.841      0.635      0.728      0.472\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Closing dataloader mosaic\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["      41/50      2.63G      1.082     0.7769      1.184          1        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 126/126 [01:03<00:00,  2.00it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:10<00:00,  1.79it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all        607       1032      0.829      0.647      0.735      0.477\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["      42/50      2.63G      1.059      0.731      1.176          2        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 126/126 [01:02<00:00,  2.03it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:10<00:00,  1.74it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all        607       1032      0.838      0.648      0.747      0.478\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["      43/50      2.63G      1.061     0.7108      1.176          2        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 126/126 [01:02<00:00,  2.00it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:10<00:00,  1.77it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all        607       1032      0.837      0.641      0.739      0.485\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["      44/50      2.63G      1.042     0.7281      1.168          4        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 126/126 [01:02<00:00,  2.01it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:10<00:00,  1.81it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all        607       1032      0.839      0.656      0.756      0.497\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["      45/50      2.63G      1.041     0.6887       1.16          2        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 126/126 [01:02<00:00,  2.00it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:10<00:00,  1.74it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all        607       1032      0.867      0.656      0.755      0.496\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["      46/50      2.63G      1.022      0.686      1.155          5        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 126/126 [01:02<00:00,  2.03it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:10<00:00,  1.76it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all        607       1032      0.833      0.656      0.754      0.498\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["      47/50      2.63G       1.03     0.6742      1.162          7        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 126/126 [01:02<00:00,  2.00it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:10<00:00,  1.83it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all        607       1032      0.859      0.646      0.754      0.498\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["      48/50      2.63G      1.004     0.6625      1.147          2        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 126/126 [01:02<00:00,  2.01it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:11<00:00,  1.72it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all        607       1032       0.83      0.673      0.753      0.497\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["      49/50      2.63G     0.9952     0.6652      1.136          2        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 126/126 [01:02<00:00,  2.03it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:10<00:00,  1.76it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all        607       1032      0.846      0.659      0.755        0.5\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["      50/50      2.63G     0.9839     0.6388       1.13          2        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 126/126 [01:02<00:00,  2.01it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:10<00:00,  1.84it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all        607       1032       0.84      0.663      0.757      0.504\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","50 epochs completed in 1.059 hours.\n","Optimizer stripped from output/train/weights/last.pt, 5.5MB\n","Optimizer stripped from output/train/weights/best.pt, 5.5MB\n","\n","Validating output/train/weights/best.pt...\n","Ultralytics 8.3.90 ğŸš€ Python-3.11.11 torch-2.5.1+cu124 CUDA:0 (Tesla T4, 15095MiB)\n","YOLO11n summary (fused): 100 layers, 2,582,542 parameters, 0 gradients, 6.3 GFLOPs\n"]},{"output_type":"stream","name":"stderr","text":["                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:13<00:00,  1.43it/s]\n"]},{"output_type":"stream","name":"stdout","text":["                   all        607       1032       0.84      0.663      0.757      0.504\n","                  fire        380        662      0.813      0.682      0.784      0.437\n","                person        227        370      0.866      0.643       0.73       0.57\n","Speed: 0.2ms preprocess, 2.5ms inference, 0.0ms loss, 1.0ms postprocess per image\n","Results saved to \u001b[1moutput/train\u001b[0m\n","\u001b[31m\u001b[1mì¶©ëŒ\u001b[0m\u001b[1m: \u001b[0míŒŒì¸íŠœë‹ì„ ì§„í–‰í•˜ë˜ ì¤‘, ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.\n","'DetMetrics' object has no attribute 'box_loss'. See valid attributes below.\n","\n","    Utility class for computing detection metrics such as precision, recall, and mean average precision (mAP).\n","\n","    Attributes:\n","        save_dir (Path): A path to the directory where the output plots will be saved.\n","        plot (bool): A flag that indicates whether to plot precision-recall curves for each class.\n","        names (dict): A dictionary of class names.\n","        box (Metric): An instance of the Metric class for storing detection results.\n","        speed (dict): A dictionary for storing execution times of different parts of the detection process.\n","        task (str): The task type, set to 'detect'.\n","    \n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run <strong style=\"color:#cdcd00\">exp_yolo11n</strong> at: <a href='https://wandb.ai/nunajiyo-kookmin-university/yolov11-finetuning/runs/p00byoqz' target=\"_blank\">https://wandb.ai/nunajiyo-kookmin-university/yolov11-finetuning/runs/p00byoqz</a><br> View project at: <a href='https://wandb.ai/nunajiyo-kookmin-university/yolov11-finetuning' target=\"_blank\">https://wandb.ai/nunajiyo-kookmin-university/yolov11-finetuning</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>./wandb/run-20250314_163252-p00byoqz/logs</code>"]},"metadata":{}},{"output_type":"error","ename":"NameError","evalue":"name 'exit' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/content/drive/MyDrive/YOLO/yolov11n-finetuning/Detectify/train.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self, dataset, output, epochs, batch_size, save_per_epochs, cache, resume)\u001b[0m\n\u001b[1;32m    102\u001b[0m             wandb.log({\n\u001b[0;32m--> 103\u001b[0;31m                 \u001b[0;34m\"train/box_loss\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbox_loss\u001b[0m\u001b[0;34m,\u001b[0m   \u001b[0;31m# ë°”ìš´ë”© ë°•ìŠ¤ ì†ì‹¤\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m                 \u001b[0;34m\"train/cls_loss\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcls_loss\u001b[0m\u001b[0;34m,\u001b[0m   \u001b[0;31m# í´ë˜ìŠ¤ ì†ì‹¤\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/utils/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"'{name}' object has no attribute '{attr}'. See valid attributes below.\\n{self.__doc__}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'DetMetrics' object has no attribute 'box_loss'. See valid attributes below.\n\n    Utility class for computing detection metrics such as precision, recall, and mean average precision (mAP).\n\n    Attributes:\n        save_dir (Path): A path to the directory where the output plots will be saved.\n        plot (bool): A flag that indicates whether to plot precision-recall curves for each class.\n        names (dict): A dictionary of class names.\n        box (Metric): An instance of the Metric class for storing detection results.\n        speed (dict): A dictionary for storing execution times of different parts of the detection process.\n        task (str): The task type, set to 'detect'.\n    ","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-c54cdff3cc8e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/YOLO/yolov11n-finetuning/data.yaml\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/content/drive/MyDrive/YOLO/yolov11n-finetuning/Detectify/train.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self, dataset, output, epochs, batch_size, save_per_epochs, cache, resume)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"íŒŒì¸íŠœë‹ì„ ì§„í–‰í•˜ë˜ ì¤‘, ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/YOLO/yolov11n-finetuning/Detectify/logger.py\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__write__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ì¶©ëŒ\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'exit' is not defined"]}]},{"cell_type":"code","source":[],"metadata":{"id":"Nmeh7GDBfygs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### ëª¨ë¸ ì¸í¼ëŸ°ìŠ¤"],"metadata":{"id":"6aJNuTmqlp_T"}},{"cell_type":"code","source":["import cv2\n","from ultralytics import YOLO\n","from google.colab.patches import cv2_imshow\n","import os"],"metadata":{"id":"FGEHMsNxnI0E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cap = cv2.VideoCapture(\"/content/drive/MyDrive/YOLO/test_input/test_input1_long_HD.mp4\")\n","\n","width = int(cap.get(3))  # ì˜ìƒ ê°€ë¡œ í¬ê¸°\n","height = int(cap.get(4))  # ì˜ìƒ ì„¸ë¡œ í¬ê¸°\n","\n","print(f\"Width: {width}, Height: {height}\")\n","cap.release()"],"metadata":{"id":"pQ1UMtz5nIvr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1742007916904,"user_tz":-540,"elapsed":1662,"user":{"displayName":"ê¹€ì˜ˆí–¥","userId":"01564514346289315086"}},"outputId":"7e7274ee-7fbe-4364-e82f-6c0fc7df89fe"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Width: 1280, Height: 720\n"]}]},{"cell_type":"code","source":["# YOLOv11 Nano ëª¨ë¸ ë¡œë“œ\n","model = YOLO(\"/content/drive/MyDrive/YOLO/yolov11n-finetuning/output/train/weights/best.pt\")\n","\n","# ì…ë ¥ ì˜ìƒ ë¡œë“œ\n","cap = cv2.VideoCapture(\"/content/drive/MyDrive/YOLO/test_input/test_input1_long_HD.mp4\")\n","\n","# í•´ìƒë„ í™•ì¸\n","width = int(cap.get(3))\n","height = int(cap.get(4))\n","fps = cap.get(5)\n","\n","if width == 0 or height == 0:\n","    print(\"âŒ Error: Video file could not be loaded. Check the file path.\")\n","    cap.release()\n","    exit()\n","\n","print(f\"âœ… Video Loaded: ({width}x{height}, {fps} FPS)\")\n","\n","# MP4ë¡œ ì €ì¥í•  ë¹„ë””ì˜¤ ì„¤ì • (ì½”ë±: mp4v)\n","fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n","output_path = \"runs/detect/predict1/test_output1_long_HD.mp4\"\n","os.makedirs(os.path.dirname(output_path), exist_ok=True)\n","\n","out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n","\n","while cap.isOpened():\n","    ret, frame = cap.read()\n","    if not ret:\n","      print(\"Video processing complete!\")\n","      break\n","\n","    # YOLO ê°ì²´ íƒì§€ ì‹¤í–‰\n","    results = model(frame)\n","\n","    # íƒì§€ ê²°ê³¼ê°€ ê·¸ë ¤ì§„ í”„ë ˆì„\n","    annotated_frame = results[0].plot()\n","\n","    # MP4 íŒŒì¼ë¡œ ì €ì¥\n","    out.write(annotated_frame)\n","\n","cap.release()\n","out.release()\n","\n","print(\"MP4 file save! \")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"T05u7fGro2GJ","executionInfo":{"status":"ok","timestamp":1742008316134,"user_tz":-540,"elapsed":374133,"user":{"displayName":"ê¹€ì˜ˆí–¥","userId":"01564514346289315086"}},"outputId":"8a79b3f6-4353-44eb-ed99-1d68935a95fb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43mìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥ ë‚´ìš©ì´ ê¸¸ì–´ì„œ ë§ˆì§€ë§‰ 5000ì¤„ì´ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.\u001b[0m\n","\n","0: 384x640 5 persons, 10.6ms\n","Speed: 2.2ms preprocess, 10.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 7 persons, 11.1ms\n","Speed: 2.1ms preprocess, 11.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 7 persons, 12.6ms\n","Speed: 2.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 7 persons, 11.6ms\n","Speed: 2.2ms preprocess, 11.6ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 8 persons, 14.3ms\n","Speed: 2.3ms preprocess, 14.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 7 persons, 14.4ms\n","Speed: 2.3ms preprocess, 14.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 7 persons, 12.2ms\n","Speed: 2.2ms preprocess, 12.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 8 persons, 13.3ms\n","Speed: 2.3ms preprocess, 13.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 8 persons, 13.2ms\n","Speed: 2.3ms preprocess, 13.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 8 persons, 12.7ms\n","Speed: 2.3ms preprocess, 12.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 persons, 13.8ms\n","Speed: 2.4ms preprocess, 13.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 9 persons, 12.3ms\n","Speed: 2.2ms preprocess, 12.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 8 persons, 14.4ms\n","Speed: 2.2ms preprocess, 14.4ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 persons, 13.0ms\n","Speed: 2.7ms preprocess, 13.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 persons, 12.4ms\n","Speed: 2.6ms preprocess, 12.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 8 persons, 15.3ms\n","Speed: 2.4ms preprocess, 15.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 9 persons, 18.8ms\n","Speed: 2.3ms preprocess, 18.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 8 persons, 16.8ms\n","Speed: 2.6ms preprocess, 16.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 7 persons, 19.3ms\n","Speed: 2.3ms preprocess, 19.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 8 persons, 14.3ms\n","Speed: 2.3ms preprocess, 14.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 8 persons, 14.6ms\n","Speed: 2.2ms preprocess, 14.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 9 persons, 15.1ms\n","Speed: 2.4ms preprocess, 15.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 9 persons, 12.3ms\n","Speed: 2.3ms preprocess, 12.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 9 persons, 13.0ms\n","Speed: 2.3ms preprocess, 13.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 10 persons, 15.1ms\n","Speed: 2.2ms preprocess, 15.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 7 persons, 13.6ms\n","Speed: 2.3ms preprocess, 13.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 8 persons, 13.5ms\n","Speed: 2.2ms preprocess, 13.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 7 persons, 12.0ms\n","Speed: 2.2ms preprocess, 12.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 8 persons, 13.8ms\n","Speed: 2.3ms preprocess, 13.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 7 persons, 14.7ms\n","Speed: 2.2ms preprocess, 14.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 8 persons, 12.8ms\n","Speed: 2.3ms preprocess, 12.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 7 persons, 11.7ms\n","Speed: 3.8ms preprocess, 11.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 7 persons, 12.4ms\n","Speed: 3.4ms preprocess, 12.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 persons, 9.8ms\n","Speed: 2.3ms preprocess, 9.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 persons, 12.4ms\n","Speed: 2.2ms preprocess, 12.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 persons, 13.6ms\n","Speed: 2.4ms preprocess, 13.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 7 persons, 13.8ms\n","Speed: 2.6ms preprocess, 13.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 12.8ms\n","Speed: 2.3ms preprocess, 12.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 13.2ms\n","Speed: 2.2ms preprocess, 13.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 11.9ms\n","Speed: 2.6ms preprocess, 11.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 11.7ms\n","Speed: 2.2ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 18.8ms\n","Speed: 2.3ms preprocess, 18.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 14.2ms\n","Speed: 2.3ms preprocess, 14.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 14.5ms\n","Speed: 2.3ms preprocess, 14.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 15.5ms\n","Speed: 2.9ms preprocess, 15.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 17.6ms\n","Speed: 2.3ms preprocess, 17.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 18.5ms\n","Speed: 2.3ms preprocess, 18.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 12.3ms\n","Speed: 2.3ms preprocess, 12.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 12.4ms\n","Speed: 2.3ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 11.7ms\n","Speed: 2.3ms preprocess, 11.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 11.8ms\n","Speed: 2.3ms preprocess, 11.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 9.9ms\n","Speed: 2.7ms preprocess, 9.9ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.8ms\n","Speed: 2.3ms preprocess, 11.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 12.7ms\n","Speed: 2.2ms preprocess, 12.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 18.8ms\n","Speed: 2.2ms preprocess, 18.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.5ms\n","Speed: 2.2ms preprocess, 13.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 10.7ms\n","Speed: 2.3ms preprocess, 10.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 13.0ms\n","Speed: 2.3ms preprocess, 13.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 12.2ms\n","Speed: 2.3ms preprocess, 12.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 7 persons, 11.3ms\n","Speed: 2.2ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 8 persons, 11.2ms\n","Speed: 4.7ms preprocess, 11.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 persons, 12.1ms\n","Speed: 2.3ms preprocess, 12.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 7 persons, 13.9ms\n","Speed: 2.4ms preprocess, 13.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 persons, 15.6ms\n","Speed: 2.3ms preprocess, 15.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 persons, 12.6ms\n","Speed: 3.4ms preprocess, 12.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 persons, 11.3ms\n","Speed: 2.2ms preprocess, 11.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 persons, 20.1ms\n","Speed: 2.3ms preprocess, 20.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 persons, 15.0ms\n","Speed: 2.3ms preprocess, 15.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 15.8ms\n","Speed: 2.3ms preprocess, 15.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 17.0ms\n","Speed: 4.7ms preprocess, 17.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 persons, 19.5ms\n","Speed: 2.4ms preprocess, 19.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 7 persons, 11.8ms\n","Speed: 3.4ms preprocess, 11.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 8 persons, 12.0ms\n","Speed: 3.0ms preprocess, 12.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 persons, 17.8ms\n","Speed: 2.3ms preprocess, 17.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 8 persons, 16.9ms\n","Speed: 2.4ms preprocess, 16.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 9 persons, 12.2ms\n","Speed: 2.3ms preprocess, 12.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 9 persons, 12.3ms\n","Speed: 2.2ms preprocess, 12.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 12 persons, 14.4ms\n","Speed: 2.3ms preprocess, 14.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 10 persons, 13.8ms\n","Speed: 2.4ms preprocess, 13.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 10 persons, 12.8ms\n","Speed: 2.3ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 9 persons, 12.9ms\n","Speed: 2.2ms preprocess, 12.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 9 persons, 13.7ms\n","Speed: 2.2ms preprocess, 13.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 9 persons, 13.7ms\n","Speed: 2.4ms preprocess, 13.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 10 persons, 13.2ms\n","Speed: 2.2ms preprocess, 13.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 9 persons, 12.2ms\n","Speed: 2.3ms preprocess, 12.2ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 9 persons, 12.9ms\n","Speed: 2.3ms preprocess, 12.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 7 persons, 15.7ms\n","Speed: 2.4ms preprocess, 15.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 8 persons, 12.3ms\n","Speed: 3.2ms preprocess, 12.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 8 persons, 16.8ms\n","Speed: 2.5ms preprocess, 16.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 8 persons, 14.0ms\n","Speed: 2.4ms preprocess, 14.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 9 persons, 12.8ms\n","Speed: 2.5ms preprocess, 12.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 8 persons, 13.4ms\n","Speed: 2.5ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 8 persons, 16.3ms\n","Speed: 2.5ms preprocess, 16.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 8 persons, 16.1ms\n","Speed: 2.4ms preprocess, 16.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 10 persons, 14.9ms\n","Speed: 2.5ms preprocess, 14.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 19.1ms\n","Speed: 4.7ms preprocess, 19.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 12.7ms\n","Speed: 2.3ms preprocess, 12.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 14.6ms\n","Speed: 2.3ms preprocess, 14.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 12.8ms\n","Speed: 2.4ms preprocess, 12.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 15.7ms\n","Speed: 2.3ms preprocess, 15.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 14.1ms\n","Speed: 2.3ms preprocess, 14.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 12.7ms\n","Speed: 2.3ms preprocess, 12.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 13.8ms\n","Speed: 2.4ms preprocess, 13.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 persons, 11.2ms\n","Speed: 2.3ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 persons, 12.3ms\n","Speed: 2.3ms preprocess, 12.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 persons, 11.3ms\n","Speed: 2.2ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 persons, 13.5ms\n","Speed: 2.2ms preprocess, 13.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 persons, 13.0ms\n","Speed: 2.2ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 12.5ms\n","Speed: 2.3ms preprocess, 12.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 13.0ms\n","Speed: 2.4ms preprocess, 13.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 12.6ms\n","Speed: 2.3ms preprocess, 12.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 13.8ms\n","Speed: 2.3ms preprocess, 13.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 12.2ms\n","Speed: 2.4ms preprocess, 12.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 14.2ms\n","Speed: 2.3ms preprocess, 14.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 19.5ms\n","Speed: 2.4ms preprocess, 19.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 10.8ms\n","Speed: 2.7ms preprocess, 10.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 14.4ms\n","Speed: 2.8ms preprocess, 14.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 14.4ms\n","Speed: 3.7ms preprocess, 14.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 15.2ms\n","Speed: 2.3ms preprocess, 15.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 16.1ms\n","Speed: 2.4ms preprocess, 16.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 16.2ms\n","Speed: 2.4ms preprocess, 16.2ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 persons, 12.8ms\n","Speed: 2.7ms preprocess, 12.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 9.4ms\n","Speed: 2.2ms preprocess, 9.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 persons, 12.4ms\n","Speed: 2.2ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 12.5ms\n","Speed: 2.3ms preprocess, 12.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 29.3ms\n","Speed: 2.5ms preprocess, 29.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 16.6ms\n","Speed: 2.3ms preprocess, 16.6ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 13.7ms\n","Speed: 2.3ms preprocess, 13.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 12.9ms\n","Speed: 2.3ms preprocess, 12.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 9.4ms\n","Speed: 6.3ms preprocess, 9.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 persons, 13.1ms\n","Speed: 3.1ms preprocess, 13.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 persons, 15.2ms\n","Speed: 2.3ms preprocess, 15.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 persons, 17.0ms\n","Speed: 2.3ms preprocess, 17.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 persons, 15.3ms\n","Speed: 2.4ms preprocess, 15.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 persons, 10.2ms\n","Speed: 3.1ms preprocess, 10.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 persons, 12.6ms\n","Speed: 2.3ms preprocess, 12.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 persons, 10.0ms\n","Speed: 2.3ms preprocess, 10.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 7 persons, 13.1ms\n","Speed: 2.4ms preprocess, 13.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 7 persons, 14.5ms\n","Speed: 2.3ms preprocess, 14.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 7 persons, 17.4ms\n","Speed: 2.4ms preprocess, 17.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 7 persons, 13.7ms\n","Speed: 2.4ms preprocess, 13.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 7 persons, 19.0ms\n","Speed: 2.4ms preprocess, 19.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 7 persons, 23.0ms\n","Speed: 2.4ms preprocess, 23.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 7 persons, 23.4ms\n","Speed: 2.4ms preprocess, 23.4ms inference, 4.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 persons, 15.6ms\n","Speed: 3.7ms preprocess, 15.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 8 persons, 16.7ms\n","Speed: 2.2ms preprocess, 16.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 8 persons, 13.5ms\n","Speed: 2.4ms preprocess, 13.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 persons, 13.1ms\n","Speed: 2.2ms preprocess, 13.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 13.2ms\n","Speed: 2.2ms preprocess, 13.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 12.3ms\n","Speed: 2.8ms preprocess, 12.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 20.2ms\n","Speed: 2.3ms preprocess, 20.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 14.2ms\n","Speed: 2.3ms preprocess, 14.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 14.2ms\n","Speed: 2.3ms preprocess, 14.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 14.6ms\n","Speed: 2.2ms preprocess, 14.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 17.3ms\n","Speed: 2.3ms preprocess, 17.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 11.8ms\n","Speed: 3.1ms preprocess, 11.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 14.9ms\n","Speed: 3.1ms preprocess, 14.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 persons, 14.1ms\n","Speed: 2.6ms preprocess, 14.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 7 persons, 13.8ms\n","Speed: 2.4ms preprocess, 13.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 7 persons, 13.4ms\n","Speed: 2.3ms preprocess, 13.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 7 persons, 12.9ms\n","Speed: 2.2ms preprocess, 12.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 7 persons, 20.5ms\n","Speed: 3.1ms preprocess, 20.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 8 persons, 14.3ms\n","Speed: 2.3ms preprocess, 14.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 8 persons, 16.2ms\n","Speed: 3.3ms preprocess, 16.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 8 persons, 15.9ms\n","Speed: 2.7ms preprocess, 15.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 8 persons, 13.1ms\n","Speed: 2.2ms preprocess, 13.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 8 persons, 12.5ms\n","Speed: 2.3ms preprocess, 12.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 9 persons, 12.4ms\n","Speed: 4.1ms preprocess, 12.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 7 persons, 13.3ms\n","Speed: 2.3ms preprocess, 13.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 9 persons, 12.2ms\n","Speed: 2.7ms preprocess, 12.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 8 persons, 13.2ms\n","Speed: 2.4ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 persons, 12.7ms\n","Speed: 2.4ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 persons, 13.1ms\n","Speed: 2.3ms preprocess, 13.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 persons, 13.3ms\n","Speed: 2.3ms preprocess, 13.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 13.9ms\n","Speed: 2.4ms preprocess, 13.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 16.5ms\n","Speed: 6.7ms preprocess, 16.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 12.2ms\n","Speed: 3.4ms preprocess, 12.2ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 14.0ms\n","Speed: 2.2ms preprocess, 14.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 17.8ms\n","Speed: 2.2ms preprocess, 17.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 17.3ms\n","Speed: 2.3ms preprocess, 17.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 18.8ms\n","Speed: 2.3ms preprocess, 18.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 23.7ms\n","Speed: 2.3ms preprocess, 23.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 17.7ms\n","Speed: 2.4ms preprocess, 17.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 13.3ms\n","Speed: 3.7ms preprocess, 13.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 23.7ms\n","Speed: 2.7ms preprocess, 23.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 19.2ms\n","Speed: 2.5ms preprocess, 19.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 24.4ms\n","Speed: 2.3ms preprocess, 24.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 14.4ms\n","Speed: 2.2ms preprocess, 14.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 12.4ms\n","Speed: 2.3ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 18.1ms\n","Speed: 2.3ms preprocess, 18.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 13.7ms\n","Speed: 2.3ms preprocess, 13.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 18.6ms\n","Speed: 2.3ms preprocess, 18.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 17.2ms\n","Speed: 2.4ms preprocess, 17.2ms inference, 4.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 15.2ms\n","Speed: 2.3ms preprocess, 15.2ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 persons, 12.7ms\n","Speed: 2.2ms preprocess, 12.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 persons, 12.6ms\n","Speed: 2.2ms preprocess, 12.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 28.5ms\n","Speed: 2.3ms preprocess, 28.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 persons, 16.0ms\n","Speed: 2.0ms preprocess, 16.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 13.0ms\n","Speed: 2.5ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 15.4ms\n","Speed: 2.3ms preprocess, 15.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 persons, 18.5ms\n","Speed: 2.2ms preprocess, 18.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 persons, 12.8ms\n","Speed: 2.3ms preprocess, 12.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 persons, 12.6ms\n","Speed: 3.1ms preprocess, 12.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 persons, 12.4ms\n","Speed: 2.3ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 persons, 18.6ms\n","Speed: 2.3ms preprocess, 18.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 7 persons, 13.6ms\n","Speed: 2.3ms preprocess, 13.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 7 persons, 16.6ms\n","Speed: 3.7ms preprocess, 16.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 7 persons, 14.8ms\n","Speed: 4.4ms preprocess, 14.8ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 7 persons, 15.6ms\n","Speed: 3.0ms preprocess, 15.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 8 persons, 17.9ms\n","Speed: 2.3ms preprocess, 17.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 7 persons, 12.7ms\n","Speed: 3.2ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 7 persons, 12.8ms\n","Speed: 2.2ms preprocess, 12.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 persons, 12.0ms\n","Speed: 2.8ms preprocess, 12.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 13.6ms\n","Speed: 2.3ms preprocess, 13.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 7 persons, 12.9ms\n","Speed: 2.3ms preprocess, 12.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 8 persons, 12.2ms\n","Speed: 3.1ms preprocess, 12.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 16.6ms\n","Speed: 3.2ms preprocess, 16.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 persons, 16.8ms\n","Speed: 2.4ms preprocess, 16.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 7 persons, 17.8ms\n","Speed: 2.4ms preprocess, 17.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 7 persons, 17.1ms\n","Speed: 2.3ms preprocess, 17.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 7 persons, 17.2ms\n","Speed: 2.2ms preprocess, 17.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 8 persons, 20.5ms\n","Speed: 2.2ms preprocess, 20.5ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 8 persons, 14.5ms\n","Speed: 2.3ms preprocess, 14.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 8 persons, 11.9ms\n","Speed: 2.2ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 7 persons, 13.4ms\n","Speed: 2.8ms preprocess, 13.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 7 persons, 15.4ms\n","Speed: 2.2ms preprocess, 15.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 persons, 11.5ms\n","Speed: 3.4ms preprocess, 11.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 9 persons, 18.2ms\n","Speed: 2.3ms preprocess, 18.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 9 persons, 18.3ms\n","Speed: 4.2ms preprocess, 18.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 7 persons, 15.9ms\n","Speed: 2.3ms preprocess, 15.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 persons, 12.5ms\n","Speed: 2.2ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 15.8ms\n","Speed: 2.3ms preprocess, 15.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 16.2ms\n","Speed: 3.3ms preprocess, 16.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 13.5ms\n","Speed: 2.2ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 7 persons, 14.5ms\n","Speed: 6.2ms preprocess, 14.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 7 persons, 19.0ms\n","Speed: 3.0ms preprocess, 19.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 12.2ms\n","Speed: 2.3ms preprocess, 12.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 7 persons, 13.1ms\n","Speed: 2.2ms preprocess, 13.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 persons, 12.0ms\n","Speed: 2.3ms preprocess, 12.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 7 persons, 26.4ms\n","Speed: 2.4ms preprocess, 26.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 16.4ms\n","Speed: 2.8ms preprocess, 16.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 8 persons, 19.9ms\n","Speed: 2.3ms preprocess, 19.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 8 persons, 14.4ms\n","Speed: 2.3ms preprocess, 14.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 persons, 20.7ms\n","Speed: 2.4ms preprocess, 20.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 7 persons, 18.7ms\n","Speed: 2.3ms preprocess, 18.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 persons, 17.9ms\n","Speed: 2.2ms preprocess, 17.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 7 persons, 17.4ms\n","Speed: 2.4ms preprocess, 17.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 persons, 25.6ms\n","Speed: 2.3ms preprocess, 25.6ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 persons, 26.3ms\n","Speed: 2.2ms preprocess, 26.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 8 persons, 16.0ms\n","Speed: 4.2ms preprocess, 16.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 8 persons, 12.2ms\n","Speed: 2.4ms preprocess, 12.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 11 persons, 14.0ms\n","Speed: 2.3ms preprocess, 14.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 10 persons, 13.4ms\n","Speed: 2.4ms preprocess, 13.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 7 persons, 16.9ms\n","Speed: 2.3ms preprocess, 16.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 7 persons, 19.5ms\n","Speed: 2.4ms preprocess, 19.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 persons, 20.7ms\n","Speed: 2.4ms preprocess, 20.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 7 persons, 19.1ms\n","Speed: 2.3ms preprocess, 19.1ms inference, 6.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 persons, 20.1ms\n","Speed: 2.2ms preprocess, 20.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 persons, 14.7ms\n","Speed: 2.3ms preprocess, 14.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 persons, 16.7ms\n","Speed: 2.3ms preprocess, 16.7ms inference, 5.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 16.1ms\n","Speed: 2.3ms preprocess, 16.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 26.2ms\n","Speed: 2.3ms preprocess, 26.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 20.6ms\n","Speed: 2.4ms preprocess, 20.6ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 18.4ms\n","Speed: 3.2ms preprocess, 18.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 15.2ms\n","Speed: 2.3ms preprocess, 15.2ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 persons, 14.8ms\n","Speed: 2.3ms preprocess, 14.8ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 persons, 31.5ms\n","Speed: 5.4ms preprocess, 31.5ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 14.5ms\n","Speed: 2.8ms preprocess, 14.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 19.8ms\n","Speed: 2.4ms preprocess, 19.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 17.9ms\n","Speed: 2.4ms preprocess, 17.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 14.9ms\n","Speed: 7.2ms preprocess, 14.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 12.4ms\n","Speed: 2.3ms preprocess, 12.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 12.0ms\n","Speed: 2.4ms preprocess, 12.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 12.2ms\n","Speed: 2.3ms preprocess, 12.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 persons, 12.7ms\n","Speed: 2.2ms preprocess, 12.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 persons, 12.7ms\n","Speed: 2.4ms preprocess, 12.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 13.2ms\n","Speed: 2.4ms preprocess, 13.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 13.6ms\n","Speed: 2.3ms preprocess, 13.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 13.2ms\n","Speed: 2.3ms preprocess, 13.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 12.6ms\n","Speed: 2.3ms preprocess, 12.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 11.7ms\n","Speed: 2.4ms preprocess, 11.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 12.3ms\n","Speed: 2.3ms preprocess, 12.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 13.9ms\n","Speed: 2.2ms preprocess, 13.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 10.1ms\n","Speed: 2.3ms preprocess, 10.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 12.4ms\n","Speed: 2.2ms preprocess, 12.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 12.9ms\n","Speed: 2.4ms preprocess, 12.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 20.9ms\n","Speed: 2.4ms preprocess, 20.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 persons, 11.0ms\n","Speed: 2.3ms preprocess, 11.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 10.7ms\n","Speed: 6.2ms preprocess, 10.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 13.0ms\n","Speed: 2.4ms preprocess, 13.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 7 persons, 12.8ms\n","Speed: 2.5ms preprocess, 12.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 7 persons, 15.8ms\n","Speed: 4.2ms preprocess, 15.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 13.8ms\n","Speed: 2.6ms preprocess, 13.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 17.4ms\n","Speed: 2.3ms preprocess, 17.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 15.7ms\n","Speed: 3.4ms preprocess, 15.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 12.9ms\n","Speed: 2.2ms preprocess, 12.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 9.8ms\n","Speed: 2.2ms preprocess, 9.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 13.6ms\n","Speed: 2.3ms preprocess, 13.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 13.2ms\n","Speed: 2.3ms preprocess, 13.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 14.0ms\n","Speed: 2.3ms preprocess, 14.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 10.3ms\n","Speed: 2.4ms preprocess, 10.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 13.6ms\n","Speed: 2.4ms preprocess, 13.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 12.2ms\n","Speed: 2.5ms preprocess, 12.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 17.8ms\n","Speed: 2.4ms preprocess, 17.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 10.0ms\n","Speed: 2.3ms preprocess, 10.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 13.1ms\n","Speed: 2.3ms preprocess, 13.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 13.5ms\n","Speed: 2.4ms preprocess, 13.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 13.4ms\n","Speed: 2.3ms preprocess, 13.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 14.1ms\n","Speed: 2.3ms preprocess, 14.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 11.5ms\n","Speed: 2.2ms preprocess, 11.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 13.5ms\n","Speed: 2.3ms preprocess, 13.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 15.1ms\n","Speed: 2.2ms preprocess, 15.1ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 12.2ms\n","Speed: 3.9ms preprocess, 12.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 12.5ms\n","Speed: 2.2ms preprocess, 12.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 12.1ms\n","Speed: 2.3ms preprocess, 12.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 13.5ms\n","Speed: 2.2ms preprocess, 13.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 13.1ms\n","Speed: 2.3ms preprocess, 13.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 16.4ms\n","Speed: 2.3ms preprocess, 16.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 17.4ms\n","Speed: 2.4ms preprocess, 17.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 20.0ms\n","Speed: 2.3ms preprocess, 20.0ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 15.3ms\n","Speed: 2.3ms preprocess, 15.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 17.6ms\n","Speed: 2.2ms preprocess, 17.6ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 21.1ms\n","Speed: 2.4ms preprocess, 21.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 11.7ms\n","Speed: 2.4ms preprocess, 11.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 11.8ms\n","Speed: 3.4ms preprocess, 11.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 16.6ms\n","Speed: 3.3ms preprocess, 16.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 13.0ms\n","Speed: 2.3ms preprocess, 13.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 14.2ms\n","Speed: 2.3ms preprocess, 14.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 13.4ms\n","Speed: 2.4ms preprocess, 13.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 11.8ms\n","Speed: 2.3ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 13.0ms\n","Speed: 2.3ms preprocess, 13.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 12.3ms\n","Speed: 2.3ms preprocess, 12.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 13.6ms\n","Speed: 2.3ms preprocess, 13.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 16.0ms\n","Speed: 2.3ms preprocess, 16.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 13.2ms\n","Speed: 2.4ms preprocess, 13.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 12.4ms\n","Speed: 2.4ms preprocess, 12.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 14.4ms\n","Speed: 3.5ms preprocess, 14.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 13.7ms\n","Speed: 2.4ms preprocess, 13.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 9.0ms\n","Speed: 2.3ms preprocess, 9.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 13.3ms\n","Speed: 2.3ms preprocess, 13.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 12.1ms\n","Speed: 2.3ms preprocess, 12.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 18.8ms\n","Speed: 2.3ms preprocess, 18.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 13.3ms\n","Speed: 2.3ms preprocess, 13.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 16.6ms\n","Speed: 2.2ms preprocess, 16.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 19.7ms\n","Speed: 2.3ms preprocess, 19.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 20.0ms\n","Speed: 2.3ms preprocess, 20.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 15.9ms\n","Speed: 2.4ms preprocess, 15.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 10.6ms\n","Speed: 2.3ms preprocess, 10.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 14.3ms\n","Speed: 5.6ms preprocess, 14.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 12.9ms\n","Speed: 2.4ms preprocess, 12.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 12.4ms\n","Speed: 2.4ms preprocess, 12.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 12.7ms\n","Speed: 2.4ms preprocess, 12.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 11.4ms\n","Speed: 2.3ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 12.2ms\n","Speed: 2.9ms preprocess, 12.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 13.4ms\n","Speed: 2.4ms preprocess, 13.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 12.8ms\n","Speed: 2.3ms preprocess, 12.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 12.5ms\n","Speed: 2.4ms preprocess, 12.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 15.9ms\n","Speed: 3.3ms preprocess, 15.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 12.7ms\n","Speed: 2.4ms preprocess, 12.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 13.2ms\n","Speed: 2.3ms preprocess, 13.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 13.3ms\n","Speed: 2.2ms preprocess, 13.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 14.3ms\n","Speed: 2.4ms preprocess, 14.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 12.6ms\n","Speed: 2.4ms preprocess, 12.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 persons, 12.1ms\n","Speed: 2.2ms preprocess, 12.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 7 persons, 9.2ms\n","Speed: 2.3ms preprocess, 9.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 persons, 15.6ms\n","Speed: 2.3ms preprocess, 15.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 13.3ms\n","Speed: 2.3ms preprocess, 13.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 12.6ms\n","Speed: 2.3ms preprocess, 12.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 19.1ms\n","Speed: 2.3ms preprocess, 19.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 16.9ms\n","Speed: 2.4ms preprocess, 16.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 17.7ms\n","Speed: 3.1ms preprocess, 17.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 17.3ms\n","Speed: 4.6ms preprocess, 17.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 13.7ms\n","Speed: 4.1ms preprocess, 13.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 13.3ms\n","Speed: 2.4ms preprocess, 13.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 14.2ms\n","Speed: 2.4ms preprocess, 14.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 13.0ms\n","Speed: 2.2ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 14.5ms\n","Speed: 2.3ms preprocess, 14.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 13.2ms\n","Speed: 2.4ms preprocess, 13.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 11.5ms\n","Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 14.8ms\n","Speed: 2.6ms preprocess, 14.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 11.3ms\n","Speed: 3.3ms preprocess, 11.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 13.5ms\n","Speed: 2.3ms preprocess, 13.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 12.6ms\n","Speed: 2.4ms preprocess, 12.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 8.7ms\n","Speed: 3.4ms preprocess, 8.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.3ms\n","Speed: 2.3ms preprocess, 13.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.8ms\n","Speed: 2.5ms preprocess, 14.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.4ms\n","Speed: 2.4ms preprocess, 13.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.8ms\n","Speed: 2.4ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.3ms\n","Speed: 2.5ms preprocess, 12.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.9ms\n","Speed: 2.4ms preprocess, 13.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.3ms\n","Speed: 2.5ms preprocess, 13.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.2ms\n","Speed: 2.5ms preprocess, 12.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 32.4ms\n","Speed: 2.6ms preprocess, 32.4ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.1ms\n","Speed: 2.3ms preprocess, 15.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 19.9ms\n","Speed: 2.3ms preprocess, 19.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.4ms\n","Speed: 2.4ms preprocess, 15.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.0ms\n","Speed: 2.3ms preprocess, 15.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 16.0ms\n","Speed: 2.4ms preprocess, 16.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 19.8ms\n","Speed: 3.9ms preprocess, 19.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.9ms\n","Speed: 7.1ms preprocess, 15.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 12.5ms\n","Speed: 2.2ms preprocess, 12.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 12.3ms\n","Speed: 2.3ms preprocess, 12.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 14.4ms\n","Speed: 2.3ms preprocess, 14.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 10.9ms\n","Speed: 3.2ms preprocess, 10.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.7ms\n","Speed: 2.5ms preprocess, 11.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.6ms\n","Speed: 2.3ms preprocess, 13.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.2ms\n","Speed: 2.9ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.0ms\n","Speed: 2.3ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.3ms\n","Speed: 2.3ms preprocess, 14.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.6ms\n","Speed: 2.3ms preprocess, 12.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.6ms\n","Speed: 2.3ms preprocess, 13.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.1ms\n","Speed: 2.3ms preprocess, 13.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.8ms\n","Speed: 2.2ms preprocess, 12.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 10.5ms\n","Speed: 2.8ms preprocess, 10.5ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 13.8ms\n","Speed: 2.2ms preprocess, 13.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 13.2ms\n","Speed: 2.3ms preprocess, 13.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 13.1ms\n","Speed: 2.3ms preprocess, 13.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 13.3ms\n","Speed: 2.3ms preprocess, 13.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 15.9ms\n","Speed: 2.3ms preprocess, 15.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 11.2ms\n","Speed: 2.6ms preprocess, 11.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 14.7ms\n","Speed: 2.3ms preprocess, 14.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 13.6ms\n","Speed: 6.3ms preprocess, 13.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 26.3ms\n","Speed: 2.3ms preprocess, 26.3ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 14.3ms\n","Speed: 2.5ms preprocess, 14.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 15.3ms\n","Speed: 2.2ms preprocess, 15.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 11.2ms\n","Speed: 2.4ms preprocess, 11.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 12.7ms\n","Speed: 2.3ms preprocess, 12.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 12.1ms\n","Speed: 2.3ms preprocess, 12.1ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 11.7ms\n","Speed: 2.3ms preprocess, 11.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 11.9ms\n","Speed: 2.3ms preprocess, 11.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 13.7ms\n","Speed: 2.3ms preprocess, 13.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 9.4ms\n","Speed: 2.3ms preprocess, 9.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 12.9ms\n","Speed: 2.3ms preprocess, 12.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 11.6ms\n","Speed: 3.6ms preprocess, 11.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 11.4ms\n","Speed: 2.3ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 14.9ms\n","Speed: 2.3ms preprocess, 14.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 14.1ms\n","Speed: 2.3ms preprocess, 14.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 10.6ms\n","Speed: 2.2ms preprocess, 10.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 12.1ms\n","Speed: 2.3ms preprocess, 12.1ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 9.8ms\n","Speed: 2.2ms preprocess, 9.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 9.2ms\n","Speed: 2.3ms preprocess, 9.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 11.0ms\n","Speed: 2.2ms preprocess, 11.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 12.9ms\n","Speed: 2.4ms preprocess, 12.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 12.0ms\n","Speed: 2.6ms preprocess, 12.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 13.3ms\n","Speed: 2.3ms preprocess, 13.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 12.9ms\n","Speed: 2.3ms preprocess, 12.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 14.1ms\n","Speed: 2.4ms preprocess, 14.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 11.5ms\n","Speed: 2.3ms preprocess, 11.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 18.0ms\n","Speed: 2.4ms preprocess, 18.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 15.7ms\n","Speed: 3.3ms preprocess, 15.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 17.6ms\n","Speed: 2.2ms preprocess, 17.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 13.7ms\n","Speed: 5.3ms preprocess, 13.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 15.7ms\n","Speed: 2.4ms preprocess, 15.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 17.1ms\n","Speed: 2.3ms preprocess, 17.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 9.4ms\n","Speed: 3.6ms preprocess, 9.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 15.2ms\n","Speed: 2.2ms preprocess, 15.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 16.3ms\n","Speed: 2.3ms preprocess, 16.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 14.5ms\n","Speed: 2.3ms preprocess, 14.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 11.0ms\n","Speed: 2.6ms preprocess, 11.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 12.9ms\n","Speed: 2.4ms preprocess, 12.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 12.9ms\n","Speed: 2.4ms preprocess, 12.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 11.7ms\n","Speed: 2.5ms preprocess, 11.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 13.3ms\n","Speed: 2.4ms preprocess, 13.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 14.1ms\n","Speed: 2.2ms preprocess, 14.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 15.0ms\n","Speed: 2.7ms preprocess, 15.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 9.6ms\n","Speed: 6.9ms preprocess, 9.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 13.6ms\n","Speed: 2.3ms preprocess, 13.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 13.3ms\n","Speed: 2.2ms preprocess, 13.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 11.2ms\n","Speed: 2.3ms preprocess, 11.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 15.5ms\n","Speed: 2.2ms preprocess, 15.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.2ms\n","Speed: 2.3ms preprocess, 14.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.2ms\n","Speed: 2.3ms preprocess, 12.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.9ms\n","Speed: 2.3ms preprocess, 12.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.4ms\n","Speed: 2.3ms preprocess, 14.4ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 17.7ms\n","Speed: 2.3ms preprocess, 17.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 17.3ms\n","Speed: 2.6ms preprocess, 17.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 15.6ms\n","Speed: 2.5ms preprocess, 15.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 19.9ms\n","Speed: 2.7ms preprocess, 19.9ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.4ms\n","Speed: 2.2ms preprocess, 13.4ms inference, 4.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.0ms\n","Speed: 5.4ms preprocess, 14.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 17.1ms\n","Speed: 2.4ms preprocess, 17.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 13.5ms\n","Speed: 2.2ms preprocess, 13.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 11.5ms\n","Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.6ms\n","Speed: 2.5ms preprocess, 12.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.4ms\n","Speed: 2.5ms preprocess, 12.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.7ms\n","Speed: 2.3ms preprocess, 13.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.4ms\n","Speed: 2.4ms preprocess, 14.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.5ms\n","Speed: 2.3ms preprocess, 15.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.9ms\n","Speed: 2.4ms preprocess, 11.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.6ms\n","Speed: 2.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.4ms\n","Speed: 2.3ms preprocess, 15.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.2ms\n","Speed: 2.3ms preprocess, 12.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.0ms\n","Speed: 2.6ms preprocess, 11.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.3ms\n","Speed: 2.3ms preprocess, 13.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 17.4ms\n","Speed: 2.5ms preprocess, 17.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.6ms\n","Speed: 2.5ms preprocess, 15.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.2ms\n","Speed: 2.4ms preprocess, 12.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.0ms\n","Speed: 2.6ms preprocess, 14.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.2ms\n","Speed: 2.6ms preprocess, 14.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.9ms\n","Speed: 2.6ms preprocess, 13.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 18.0ms\n","Speed: 4.8ms preprocess, 18.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 16.2ms\n","Speed: 2.4ms preprocess, 16.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 16.1ms\n","Speed: 2.7ms preprocess, 16.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 17.3ms\n","Speed: 2.3ms preprocess, 17.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 16.3ms\n","Speed: 2.3ms preprocess, 16.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 16.6ms\n","Speed: 2.5ms preprocess, 16.6ms inference, 7.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.4ms\n","Speed: 2.4ms preprocess, 15.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.2ms\n","Speed: 2.4ms preprocess, 12.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.5ms\n","Speed: 2.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.7ms\n","Speed: 2.3ms preprocess, 11.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.6ms\n","Speed: 2.3ms preprocess, 13.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.8ms\n","Speed: 2.2ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.8ms\n","Speed: 2.3ms preprocess, 11.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.8ms\n","Speed: 2.3ms preprocess, 12.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.2ms\n","Speed: 2.2ms preprocess, 12.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.1ms\n","Speed: 2.3ms preprocess, 13.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.0ms\n","Speed: 2.3ms preprocess, 13.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.5ms\n","Speed: 2.2ms preprocess, 13.5ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.8ms\n","Speed: 2.3ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.8ms\n","Speed: 2.2ms preprocess, 12.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.6ms\n","Speed: 2.3ms preprocess, 11.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.7ms\n","Speed: 2.2ms preprocess, 12.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.3ms\n","Speed: 2.2ms preprocess, 14.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.9ms\n","Speed: 2.2ms preprocess, 15.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.3ms\n","Speed: 2.2ms preprocess, 12.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 9.9ms\n","Speed: 2.4ms preprocess, 9.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.4ms\n","Speed: 2.2ms preprocess, 13.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.8ms\n","Speed: 2.2ms preprocess, 14.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.5ms\n","Speed: 3.5ms preprocess, 14.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 20.1ms\n","Speed: 3.1ms preprocess, 20.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 20.2ms\n","Speed: 3.1ms preprocess, 20.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 23.5ms\n","Speed: 2.4ms preprocess, 23.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.1ms\n","Speed: 2.3ms preprocess, 14.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 23.8ms\n","Speed: 2.9ms preprocess, 23.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 17.7ms\n","Speed: 2.2ms preprocess, 17.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 20.9ms\n","Speed: 2.4ms preprocess, 20.9ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 17.8ms\n","Speed: 2.3ms preprocess, 17.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.3ms\n","Speed: 2.3ms preprocess, 14.3ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 16.7ms\n","Speed: 2.3ms preprocess, 16.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.1ms\n","Speed: 2.3ms preprocess, 15.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 21.9ms\n","Speed: 2.4ms preprocess, 21.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 17.8ms\n","Speed: 2.4ms preprocess, 17.8ms inference, 5.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 18.6ms\n","Speed: 2.4ms preprocess, 18.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 16.5ms\n","Speed: 2.1ms preprocess, 16.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.1ms\n","Speed: 2.2ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.8ms\n","Speed: 2.3ms preprocess, 14.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.0ms\n","Speed: 2.1ms preprocess, 14.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 18.0ms\n","Speed: 2.3ms preprocess, 18.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.1ms\n","Speed: 2.2ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 20.6ms\n","Speed: 2.2ms preprocess, 20.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 16.8ms\n","Speed: 4.7ms preprocess, 16.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 24.8ms\n","Speed: 2.3ms preprocess, 24.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 19.2ms\n","Speed: 2.5ms preprocess, 19.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 16.9ms\n","Speed: 2.3ms preprocess, 16.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 17.1ms\n","Speed: 5.4ms preprocess, 17.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 11.9ms\n","Speed: 3.3ms preprocess, 11.9ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 13.0ms\n","Speed: 2.3ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 20.8ms\n","Speed: 2.2ms preprocess, 20.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 15.6ms\n","Speed: 2.3ms preprocess, 15.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 12.9ms\n","Speed: 2.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 11.5ms\n","Speed: 2.3ms preprocess, 11.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 11.8ms\n","Speed: 2.7ms preprocess, 11.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 12.4ms\n","Speed: 2.3ms preprocess, 12.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 12.0ms\n","Speed: 2.3ms preprocess, 12.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.8ms\n","Speed: 2.3ms preprocess, 15.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.9ms\n","Speed: 2.1ms preprocess, 14.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 19.9ms\n","Speed: 2.3ms preprocess, 19.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.1ms\n","Speed: 2.3ms preprocess, 11.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.8ms\n","Speed: 2.2ms preprocess, 12.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.4ms\n","Speed: 2.2ms preprocess, 12.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 28.5ms\n","Speed: 2.5ms preprocess, 28.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 21.5ms\n","Speed: 3.2ms preprocess, 21.5ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 20.5ms\n","Speed: 3.8ms preprocess, 20.5ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 30.0ms\n","Speed: 2.4ms preprocess, 30.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 20.3ms\n","Speed: 2.4ms preprocess, 20.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 13.4ms\n","Speed: 2.3ms preprocess, 13.4ms inference, 5.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 17.7ms\n","Speed: 2.4ms preprocess, 17.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 24.3ms\n","Speed: 2.3ms preprocess, 24.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 13.8ms\n","Speed: 2.4ms preprocess, 13.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.6ms\n","Speed: 2.4ms preprocess, 15.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 13.3ms\n","Speed: 2.3ms preprocess, 13.3ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.0ms\n","Speed: 2.6ms preprocess, 15.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.9ms\n","Speed: 3.0ms preprocess, 11.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 19.7ms\n","Speed: 2.3ms preprocess, 19.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 20.0ms\n","Speed: 2.2ms preprocess, 20.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.1ms\n","Speed: 2.5ms preprocess, 13.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.2ms\n","Speed: 2.3ms preprocess, 13.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.0ms\n","Speed: 2.2ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.9ms\n","Speed: 2.3ms preprocess, 12.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.7ms\n","Speed: 2.3ms preprocess, 13.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.8ms\n","Speed: 2.3ms preprocess, 13.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.7ms\n","Speed: 2.5ms preprocess, 13.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 18.2ms\n","Speed: 2.3ms preprocess, 18.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 26.1ms\n","Speed: 2.3ms preprocess, 26.1ms inference, 5.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 20.4ms\n","Speed: 2.2ms preprocess, 20.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 17.0ms\n","Speed: 2.5ms preprocess, 17.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.7ms\n","Speed: 2.2ms preprocess, 13.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.0ms\n","Speed: 2.9ms preprocess, 12.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.6ms\n","Speed: 2.2ms preprocess, 12.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.6ms\n","Speed: 5.2ms preprocess, 14.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.6ms\n","Speed: 2.5ms preprocess, 12.6ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.5ms\n","Speed: 5.9ms preprocess, 14.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.6ms\n","Speed: 2.4ms preprocess, 12.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.3ms\n","Speed: 3.0ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 22.9ms\n","Speed: 2.3ms preprocess, 22.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.4ms\n","Speed: 2.3ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 18.8ms\n","Speed: 3.5ms preprocess, 18.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 21.5ms\n","Speed: 2.4ms preprocess, 21.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.9ms\n","Speed: 2.2ms preprocess, 14.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.0ms\n","Speed: 2.1ms preprocess, 15.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 20.9ms\n","Speed: 2.3ms preprocess, 20.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 16.4ms\n","Speed: 5.4ms preprocess, 16.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 16.3ms\n","Speed: 2.4ms preprocess, 16.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 20.2ms\n","Speed: 2.2ms preprocess, 20.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 20.1ms\n","Speed: 2.6ms preprocess, 20.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 20.3ms\n","Speed: 2.3ms preprocess, 20.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 32.6ms\n","Speed: 2.5ms preprocess, 32.6ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 18.4ms\n","Speed: 2.3ms preprocess, 18.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 16.3ms\n","Speed: 5.3ms preprocess, 16.3ms inference, 6.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.8ms\n","Speed: 5.3ms preprocess, 15.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.3ms\n","Speed: 5.3ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.0ms\n","Speed: 2.5ms preprocess, 14.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 19.3ms\n","Speed: 2.4ms preprocess, 19.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 19.7ms\n","Speed: 2.3ms preprocess, 19.7ms inference, 5.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 22.6ms\n","Speed: 2.3ms preprocess, 22.6ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 16.0ms\n","Speed: 2.3ms preprocess, 16.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 16.6ms\n","Speed: 2.5ms preprocess, 16.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 18.1ms\n","Speed: 2.3ms preprocess, 18.1ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 18.9ms\n","Speed: 2.4ms preprocess, 18.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 18.3ms\n","Speed: 2.8ms preprocess, 18.3ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 24.6ms\n","Speed: 2.4ms preprocess, 24.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 18.3ms\n","Speed: 2.5ms preprocess, 18.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 18.1ms\n","Speed: 3.7ms preprocess, 18.1ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.8ms\n","Speed: 3.0ms preprocess, 15.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 19.4ms\n","Speed: 3.0ms preprocess, 19.4ms inference, 5.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 18.1ms\n","Speed: 2.4ms preprocess, 18.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 18.8ms\n","Speed: 2.4ms preprocess, 18.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 22.7ms\n","Speed: 2.4ms preprocess, 22.7ms inference, 6.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 16.5ms\n","Speed: 3.2ms preprocess, 16.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 18.0ms\n","Speed: 2.3ms preprocess, 18.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.6ms\n","Speed: 2.5ms preprocess, 14.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.2ms\n","Speed: 2.4ms preprocess, 14.2ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 18.9ms\n","Speed: 2.6ms preprocess, 18.9ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 22.3ms\n","Speed: 2.3ms preprocess, 22.3ms inference, 5.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 17.4ms\n","Speed: 2.4ms preprocess, 17.4ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 25.1ms\n","Speed: 2.3ms preprocess, 25.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.4ms\n","Speed: 3.8ms preprocess, 15.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 25.8ms\n","Speed: 2.3ms preprocess, 25.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 10.1ms\n","Speed: 2.3ms preprocess, 10.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.9ms\n","Speed: 2.5ms preprocess, 14.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.2ms\n","Speed: 2.3ms preprocess, 12.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.5ms\n","Speed: 2.4ms preprocess, 13.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.8ms\n","Speed: 3.3ms preprocess, 15.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.0ms\n","Speed: 2.3ms preprocess, 14.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.6ms\n","Speed: 2.1ms preprocess, 13.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 17.4ms\n","Speed: 4.1ms preprocess, 17.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.0ms\n","Speed: 3.0ms preprocess, 15.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.1ms\n","Speed: 2.2ms preprocess, 12.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.8ms\n","Speed: 4.2ms preprocess, 11.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.6ms\n","Speed: 2.3ms preprocess, 13.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.2ms\n","Speed: 3.0ms preprocess, 11.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.8ms\n","Speed: 3.0ms preprocess, 11.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.4ms\n","Speed: 2.3ms preprocess, 12.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.2ms\n","Speed: 2.3ms preprocess, 13.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.4ms\n","Speed: 2.3ms preprocess, 12.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.0ms\n","Speed: 2.3ms preprocess, 12.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.7ms\n","Speed: 2.4ms preprocess, 13.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.9ms\n","Speed: 3.2ms preprocess, 12.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.9ms\n","Speed: 3.0ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.1ms\n","Speed: 2.4ms preprocess, 15.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 17.3ms\n","Speed: 2.3ms preprocess, 17.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.0ms\n","Speed: 2.3ms preprocess, 12.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 18.2ms\n","Speed: 2.2ms preprocess, 18.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.1ms\n","Speed: 2.5ms preprocess, 11.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.0ms\n","Speed: 2.2ms preprocess, 13.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.4ms\n","Speed: 2.3ms preprocess, 14.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.7ms\n","Speed: 2.2ms preprocess, 14.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.1ms\n","Speed: 2.3ms preprocess, 12.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.4ms\n","Speed: 2.5ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.5ms\n","Speed: 2.5ms preprocess, 14.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 19.0ms\n","Speed: 2.4ms preprocess, 19.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.1ms\n","Speed: 2.3ms preprocess, 15.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.8ms\n","Speed: 2.5ms preprocess, 15.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.8ms\n","Speed: 2.3ms preprocess, 15.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.9ms\n","Speed: 2.3ms preprocess, 12.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 20.7ms\n","Speed: 1.9ms preprocess, 20.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.0ms\n","Speed: 2.3ms preprocess, 15.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.8ms\n","Speed: 2.3ms preprocess, 12.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.8ms\n","Speed: 2.3ms preprocess, 11.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.5ms\n","Speed: 2.2ms preprocess, 13.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 10.2ms\n","Speed: 2.5ms preprocess, 10.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 10.7ms\n","Speed: 2.3ms preprocess, 10.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.2ms\n","Speed: 2.3ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.3ms\n","Speed: 2.4ms preprocess, 14.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.5ms\n","Speed: 2.3ms preprocess, 12.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.6ms\n","Speed: 2.5ms preprocess, 14.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.3ms\n","Speed: 2.3ms preprocess, 13.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.3ms\n","Speed: 2.2ms preprocess, 14.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.0ms\n","Speed: 2.4ms preprocess, 14.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.1ms\n","Speed: 2.3ms preprocess, 13.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 16.0ms\n","Speed: 2.4ms preprocess, 16.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.5ms\n","Speed: 2.4ms preprocess, 14.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 16.2ms\n","Speed: 2.4ms preprocess, 16.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.6ms\n","Speed: 2.3ms preprocess, 11.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.6ms\n","Speed: 2.3ms preprocess, 12.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.9ms\n","Speed: 2.3ms preprocess, 14.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.7ms\n","Speed: 2.2ms preprocess, 14.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 17.8ms\n","Speed: 2.5ms preprocess, 17.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.5ms\n","Speed: 6.6ms preprocess, 13.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.8ms\n","Speed: 7.3ms preprocess, 13.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.3ms\n","Speed: 2.4ms preprocess, 14.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.4ms\n","Speed: 2.2ms preprocess, 11.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.3ms\n","Speed: 2.3ms preprocess, 15.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 16.9ms\n","Speed: 2.3ms preprocess, 16.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.1ms\n","Speed: 2.7ms preprocess, 15.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 16.5ms\n","Speed: 3.9ms preprocess, 16.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.4ms\n","Speed: 2.4ms preprocess, 13.4ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 16.4ms\n","Speed: 2.2ms preprocess, 16.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.4ms\n","Speed: 2.3ms preprocess, 13.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.6ms\n","Speed: 2.3ms preprocess, 11.6ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 10.7ms\n","Speed: 2.2ms preprocess, 10.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.4ms\n","Speed: 2.3ms preprocess, 13.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 10.3ms\n","Speed: 2.6ms preprocess, 10.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.9ms\n","Speed: 2.3ms preprocess, 13.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.5ms\n","Speed: 2.3ms preprocess, 15.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.9ms\n","Speed: 2.3ms preprocess, 13.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.0ms\n","Speed: 2.3ms preprocess, 13.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.3ms\n","Speed: 2.3ms preprocess, 14.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.1ms\n","Speed: 2.4ms preprocess, 14.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 10.0ms\n","Speed: 2.2ms preprocess, 10.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.0ms\n","Speed: 2.5ms preprocess, 14.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.8ms\n","Speed: 2.9ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 17.1ms\n","Speed: 3.6ms preprocess, 17.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.4ms\n","Speed: 2.2ms preprocess, 15.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.8ms\n","Speed: 3.1ms preprocess, 14.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 16.0ms\n","Speed: 2.3ms preprocess, 16.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.2ms\n","Speed: 2.2ms preprocess, 12.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.9ms\n","Speed: 2.9ms preprocess, 13.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.9ms\n","Speed: 2.3ms preprocess, 12.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.6ms\n","Speed: 2.3ms preprocess, 12.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.8ms\n","Speed: 2.2ms preprocess, 11.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.6ms\n","Speed: 2.2ms preprocess, 12.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 17.4ms\n","Speed: 2.4ms preprocess, 17.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.7ms\n","Speed: 2.3ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.9ms\n","Speed: 2.3ms preprocess, 12.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.1ms\n","Speed: 2.2ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.3ms\n","Speed: 2.4ms preprocess, 14.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.6ms\n","Speed: 2.3ms preprocess, 12.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 10.3ms\n","Speed: 2.3ms preprocess, 10.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.2ms\n","Speed: 2.2ms preprocess, 12.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.7ms\n","Speed: 2.2ms preprocess, 11.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.6ms\n","Speed: 2.2ms preprocess, 13.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.0ms\n","Speed: 2.4ms preprocess, 11.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.3ms\n","Speed: 2.4ms preprocess, 14.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.7ms\n","Speed: 2.2ms preprocess, 11.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.4ms\n","Speed: 2.4ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.1ms\n","Speed: 2.3ms preprocess, 14.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.5ms\n","Speed: 2.3ms preprocess, 13.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.8ms\n","Speed: 2.3ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 19.8ms\n","Speed: 2.4ms preprocess, 19.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.8ms\n","Speed: 2.3ms preprocess, 15.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 16.9ms\n","Speed: 3.8ms preprocess, 16.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.8ms\n","Speed: 2.4ms preprocess, 15.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.0ms\n","Speed: 2.4ms preprocess, 15.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.3ms\n","Speed: 2.2ms preprocess, 11.3ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.2ms\n","Speed: 2.3ms preprocess, 11.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.0ms\n","Speed: 2.2ms preprocess, 13.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.2ms\n","Speed: 2.3ms preprocess, 13.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.0ms\n","Speed: 2.5ms preprocess, 13.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.5ms\n","Speed: 2.3ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.2ms\n","Speed: 2.2ms preprocess, 12.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.5ms\n","Speed: 2.2ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.0ms\n","Speed: 2.3ms preprocess, 11.0ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.1ms\n","Speed: 2.3ms preprocess, 13.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.6ms\n","Speed: 2.2ms preprocess, 11.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.5ms\n","Speed: 2.3ms preprocess, 13.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.8ms\n","Speed: 2.5ms preprocess, 11.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.0ms\n","Speed: 2.2ms preprocess, 12.0ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.5ms\n","Speed: 2.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 16.3ms\n","Speed: 2.3ms preprocess, 16.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.4ms\n","Speed: 2.3ms preprocess, 13.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 10.5ms\n","Speed: 2.6ms preprocess, 10.5ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.8ms\n","Speed: 2.4ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.0ms\n","Speed: 2.3ms preprocess, 15.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.0ms\n","Speed: 2.9ms preprocess, 12.0ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.7ms\n","Speed: 2.2ms preprocess, 11.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 16.0ms\n","Speed: 4.1ms preprocess, 16.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.2ms\n","Speed: 2.7ms preprocess, 15.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.4ms\n","Speed: 4.0ms preprocess, 14.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.3ms\n","Speed: 2.3ms preprocess, 14.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 20.2ms\n","Speed: 2.3ms preprocess, 20.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.4ms\n","Speed: 2.3ms preprocess, 14.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.9ms\n","Speed: 2.2ms preprocess, 14.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 10.8ms\n","Speed: 2.2ms preprocess, 10.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.3ms\n","Speed: 2.2ms preprocess, 12.3ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.9ms\n","Speed: 2.2ms preprocess, 11.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 19.3ms\n","Speed: 2.3ms preprocess, 19.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 9.6ms\n","Speed: 2.4ms preprocess, 9.6ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.7ms\n","Speed: 2.8ms preprocess, 11.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.8ms\n","Speed: 2.3ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 17.0ms\n","Speed: 2.2ms preprocess, 17.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.3ms\n","Speed: 2.2ms preprocess, 12.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.0ms\n","Speed: 2.3ms preprocess, 12.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.3ms\n","Speed: 2.2ms preprocess, 13.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.0ms\n","Speed: 2.2ms preprocess, 13.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.9ms\n","Speed: 2.2ms preprocess, 12.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.2ms\n","Speed: 2.3ms preprocess, 13.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.5ms\n","Speed: 2.3ms preprocess, 11.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.6ms\n","Speed: 2.3ms preprocess, 13.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 18.7ms\n","Speed: 2.3ms preprocess, 18.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.2ms\n","Speed: 5.0ms preprocess, 13.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 16.4ms\n","Speed: 2.3ms preprocess, 16.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.5ms\n","Speed: 2.3ms preprocess, 13.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 16.4ms\n","Speed: 2.2ms preprocess, 16.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 18.3ms\n","Speed: 2.3ms preprocess, 18.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 20.3ms\n","Speed: 2.3ms preprocess, 20.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 16.3ms\n","Speed: 2.3ms preprocess, 16.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 18.6ms\n","Speed: 3.6ms preprocess, 18.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.3ms\n","Speed: 2.3ms preprocess, 12.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.0ms\n","Speed: 2.2ms preprocess, 12.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.4ms\n","Speed: 2.3ms preprocess, 11.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 10.9ms\n","Speed: 2.3ms preprocess, 10.9ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.9ms\n","Speed: 2.4ms preprocess, 13.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.2ms\n","Speed: 3.0ms preprocess, 13.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.8ms\n","Speed: 2.2ms preprocess, 12.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.1ms\n","Speed: 2.2ms preprocess, 15.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.4ms\n","Speed: 2.4ms preprocess, 12.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.9ms\n","Speed: 2.2ms preprocess, 13.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.5ms\n","Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.0ms\n","Speed: 2.3ms preprocess, 12.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.4ms\n","Speed: 2.2ms preprocess, 14.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.5ms\n","Speed: 2.2ms preprocess, 11.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 10.1ms\n","Speed: 2.2ms preprocess, 10.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.7ms\n","Speed: 2.2ms preprocess, 14.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.2ms\n","Speed: 2.3ms preprocess, 13.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.1ms\n","Speed: 2.3ms preprocess, 12.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 10.8ms\n","Speed: 2.5ms preprocess, 10.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.9ms\n","Speed: 2.2ms preprocess, 11.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.9ms\n","Speed: 2.2ms preprocess, 12.9ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 21.5ms\n","Speed: 2.2ms preprocess, 21.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.0ms\n","Speed: 2.2ms preprocess, 14.0ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.2ms\n","Speed: 3.0ms preprocess, 15.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 20.7ms\n","Speed: 2.3ms preprocess, 20.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.5ms\n","Speed: 2.9ms preprocess, 12.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.7ms\n","Speed: 2.7ms preprocess, 13.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.4ms\n","Speed: 2.3ms preprocess, 15.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 16.1ms\n","Speed: 2.2ms preprocess, 16.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.9ms\n","Speed: 2.3ms preprocess, 12.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.6ms\n","Speed: 3.4ms preprocess, 13.6ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.6ms\n","Speed: 2.3ms preprocess, 13.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.8ms\n","Speed: 2.2ms preprocess, 11.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.2ms\n","Speed: 2.4ms preprocess, 12.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.8ms\n","Speed: 2.2ms preprocess, 13.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.6ms\n","Speed: 2.2ms preprocess, 12.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.0ms\n","Speed: 2.2ms preprocess, 13.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.3ms\n","Speed: 2.3ms preprocess, 12.3ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.0ms\n","Speed: 2.2ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.1ms\n","Speed: 2.8ms preprocess, 12.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.6ms\n","Speed: 2.2ms preprocess, 13.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.5ms\n","Speed: 2.2ms preprocess, 12.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.4ms\n","Speed: 2.3ms preprocess, 11.4ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.7ms\n","Speed: 2.2ms preprocess, 12.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.4ms\n","Speed: 2.3ms preprocess, 12.4ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.1ms\n","Speed: 2.3ms preprocess, 13.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.0ms\n","Speed: 2.2ms preprocess, 15.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.2ms\n","Speed: 2.3ms preprocess, 13.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.5ms\n","Speed: 4.5ms preprocess, 12.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.5ms\n","Speed: 3.0ms preprocess, 11.5ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 19.8ms\n","Speed: 2.3ms preprocess, 19.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.1ms\n","Speed: 2.3ms preprocess, 15.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 18.3ms\n","Speed: 2.4ms preprocess, 18.3ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.5ms\n","Speed: 2.6ms preprocess, 15.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 18.6ms\n","Speed: 3.4ms preprocess, 18.6ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 9.2ms\n","Speed: 2.7ms preprocess, 9.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.1ms\n","Speed: 2.5ms preprocess, 12.1ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 16.1ms\n","Speed: 2.2ms preprocess, 16.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.1ms\n","Speed: 2.3ms preprocess, 13.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 9.4ms\n","Speed: 3.1ms preprocess, 9.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.4ms\n","Speed: 2.2ms preprocess, 12.4ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.6ms\n","Speed: 2.4ms preprocess, 11.6ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 10.0ms\n","Speed: 2.2ms preprocess, 10.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.4ms\n","Speed: 2.4ms preprocess, 14.4ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.8ms\n","Speed: 2.3ms preprocess, 12.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.6ms\n","Speed: 2.3ms preprocess, 13.6ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.3ms\n","Speed: 2.3ms preprocess, 12.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.5ms\n","Speed: 2.3ms preprocess, 12.5ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.3ms\n","Speed: 2.3ms preprocess, 11.3ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.6ms\n","Speed: 2.3ms preprocess, 12.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.0ms\n","Speed: 2.2ms preprocess, 13.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.9ms\n","Speed: 2.3ms preprocess, 11.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.9ms\n","Speed: 2.3ms preprocess, 14.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.0ms\n","Speed: 2.3ms preprocess, 13.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.4ms\n","Speed: 2.3ms preprocess, 13.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.8ms\n","Speed: 2.3ms preprocess, 11.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.6ms\n","Speed: 2.3ms preprocess, 11.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.4ms\n","Speed: 2.3ms preprocess, 12.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.2ms\n","Speed: 2.3ms preprocess, 15.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.4ms\n","Speed: 9.1ms preprocess, 15.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.3ms\n","Speed: 2.4ms preprocess, 15.3ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.4ms\n","Speed: 4.2ms preprocess, 15.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.4ms\n","Speed: 2.3ms preprocess, 15.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 17.0ms\n","Speed: 2.5ms preprocess, 17.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.7ms\n","Speed: 2.4ms preprocess, 11.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.8ms\n","Speed: 2.2ms preprocess, 11.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.7ms\n","Speed: 2.2ms preprocess, 12.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 10.0ms\n","Speed: 4.3ms preprocess, 10.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.9ms\n","Speed: 2.8ms preprocess, 12.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.3ms\n","Speed: 3.2ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.5ms\n","Speed: 2.3ms preprocess, 14.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.6ms\n","Speed: 2.3ms preprocess, 13.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.4ms\n","Speed: 2.3ms preprocess, 11.4ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.1ms\n","Speed: 2.2ms preprocess, 13.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.4ms\n","Speed: 2.2ms preprocess, 13.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.5ms\n","Speed: 2.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 10.1ms\n","Speed: 2.2ms preprocess, 10.1ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.5ms\n","Speed: 2.3ms preprocess, 12.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.0ms\n","Speed: 2.3ms preprocess, 13.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.2ms\n","Speed: 2.3ms preprocess, 13.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 9.2ms\n","Speed: 2.0ms preprocess, 9.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.6ms\n","Speed: 2.3ms preprocess, 14.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 21.6ms\n","Speed: 2.3ms preprocess, 21.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 17.7ms\n","Speed: 2.3ms preprocess, 17.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 17.6ms\n","Speed: 7.1ms preprocess, 17.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.8ms\n","Speed: 2.4ms preprocess, 15.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 24.5ms\n","Speed: 2.3ms preprocess, 24.5ms inference, 4.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 30.4ms\n","Speed: 2.8ms preprocess, 30.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 20.0ms\n","Speed: 4.4ms preprocess, 20.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.0ms\n","Speed: 2.3ms preprocess, 13.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 18.8ms\n","Speed: 2.4ms preprocess, 18.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.5ms\n","Speed: 2.2ms preprocess, 11.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.6ms\n","Speed: 2.3ms preprocess, 11.6ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.4ms\n","Speed: 2.2ms preprocess, 11.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.7ms\n","Speed: 2.2ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.6ms\n","Speed: 4.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.2ms\n","Speed: 2.2ms preprocess, 12.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.5ms\n","Speed: 2.1ms preprocess, 15.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.3ms\n","Speed: 2.3ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.3ms\n","Speed: 2.1ms preprocess, 14.3ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.0ms\n","Speed: 2.4ms preprocess, 15.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.2ms\n","Speed: 2.1ms preprocess, 12.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.5ms\n","Speed: 2.4ms preprocess, 12.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.3ms\n","Speed: 2.2ms preprocess, 13.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.9ms\n","Speed: 2.4ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.7ms\n","Speed: 2.1ms preprocess, 14.7ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.2ms\n","Speed: 2.4ms preprocess, 14.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.8ms\n","Speed: 7.7ms preprocess, 15.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.1ms\n","Speed: 4.1ms preprocess, 14.1ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 17.8ms\n","Speed: 2.4ms preprocess, 17.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 18.2ms\n","Speed: 2.3ms preprocess, 18.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 24.9ms\n","Speed: 2.8ms preprocess, 24.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 16.8ms\n","Speed: 2.3ms preprocess, 16.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 19.7ms\n","Speed: 2.3ms preprocess, 19.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.9ms\n","Speed: 2.2ms preprocess, 13.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.6ms\n","Speed: 2.2ms preprocess, 11.6ms inference, 5.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 19.2ms\n","Speed: 2.2ms preprocess, 19.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.2ms\n","Speed: 2.2ms preprocess, 13.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.3ms\n","Speed: 3.2ms preprocess, 11.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.9ms\n","Speed: 3.5ms preprocess, 14.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 23.2ms\n","Speed: 9.3ms preprocess, 23.2ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.5ms\n","Speed: 3.2ms preprocess, 12.5ms inference, 4.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 17.2ms\n","Speed: 2.3ms preprocess, 17.2ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 18.6ms\n","Speed: 2.4ms preprocess, 18.6ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 16.0ms\n","Speed: 2.6ms preprocess, 16.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 16.3ms\n","Speed: 2.2ms preprocess, 16.3ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.6ms\n","Speed: 2.2ms preprocess, 15.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.9ms\n","Speed: 2.3ms preprocess, 14.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 17.0ms\n","Speed: 2.3ms preprocess, 17.0ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 22.0ms\n","Speed: 3.3ms preprocess, 22.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 22.2ms\n","Speed: 2.3ms preprocess, 22.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.7ms\n","Speed: 2.4ms preprocess, 15.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.9ms\n","Speed: 2.4ms preprocess, 13.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 16.8ms\n","Speed: 2.4ms preprocess, 16.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 17.7ms\n","Speed: 2.5ms preprocess, 17.7ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 18.3ms\n","Speed: 4.3ms preprocess, 18.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 16.0ms\n","Speed: 2.3ms preprocess, 16.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.8ms\n","Speed: 2.1ms preprocess, 14.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 20.2ms\n","Speed: 2.9ms preprocess, 20.2ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 18.8ms\n","Speed: 3.4ms preprocess, 18.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.1ms\n","Speed: 2.3ms preprocess, 14.1ms inference, 6.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 17.3ms\n","Speed: 2.3ms preprocess, 17.3ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 20.5ms\n","Speed: 2.2ms preprocess, 20.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.8ms\n","Speed: 2.4ms preprocess, 11.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 17.2ms\n","Speed: 2.2ms preprocess, 17.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 23.6ms\n","Speed: 2.1ms preprocess, 23.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 25.3ms\n","Speed: 2.3ms preprocess, 25.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 17.1ms\n","Speed: 2.3ms preprocess, 17.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.3ms\n","Speed: 2.3ms preprocess, 14.3ms inference, 5.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 18.0ms\n","Speed: 2.2ms preprocess, 18.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 23.2ms\n","Speed: 3.3ms preprocess, 23.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 20.7ms\n","Speed: 6.7ms preprocess, 20.7ms inference, 5.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 24.1ms\n","Speed: 3.7ms preprocess, 24.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 20.4ms\n","Speed: 2.2ms preprocess, 20.4ms inference, 7.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 20.8ms\n","Speed: 2.2ms preprocess, 20.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 25.9ms\n","Speed: 2.1ms preprocess, 25.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 19.9ms\n","Speed: 2.3ms preprocess, 19.9ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.5ms\n","Speed: 2.3ms preprocess, 15.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 21.7ms\n","Speed: 2.2ms preprocess, 21.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.5ms\n","Speed: 2.2ms preprocess, 12.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.5ms\n","Speed: 2.2ms preprocess, 12.5ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.9ms\n","Speed: 2.2ms preprocess, 12.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 16.4ms\n","Speed: 2.3ms preprocess, 16.4ms inference, 5.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.7ms\n","Speed: 2.2ms preprocess, 11.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.7ms\n","Speed: 2.3ms preprocess, 12.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.7ms\n","Speed: 5.7ms preprocess, 13.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.6ms\n","Speed: 2.3ms preprocess, 13.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.8ms\n","Speed: 2.3ms preprocess, 15.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.4ms\n","Speed: 3.6ms preprocess, 13.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.5ms\n","Speed: 2.2ms preprocess, 15.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.6ms\n","Speed: 3.1ms preprocess, 11.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.7ms\n","Speed: 2.3ms preprocess, 12.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 17.8ms\n","Speed: 2.3ms preprocess, 17.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 23.6ms\n","Speed: 4.3ms preprocess, 23.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 17.0ms\n","Speed: 5.1ms preprocess, 17.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 17.2ms\n","Speed: 3.6ms preprocess, 17.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.7ms\n","Speed: 2.4ms preprocess, 14.7ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 22.7ms\n","Speed: 2.3ms preprocess, 22.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 19.1ms\n","Speed: 2.3ms preprocess, 19.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 23.1ms\n","Speed: 3.2ms preprocess, 23.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 17.7ms\n","Speed: 3.3ms preprocess, 17.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.0ms\n","Speed: 2.3ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.9ms\n","Speed: 2.2ms preprocess, 12.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 25.0ms\n","Speed: 2.4ms preprocess, 25.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.0ms\n","Speed: 2.5ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.7ms\n","Speed: 2.2ms preprocess, 12.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.8ms\n","Speed: 2.8ms preprocess, 14.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 21.6ms\n","Speed: 2.3ms preprocess, 21.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 19.2ms\n","Speed: 2.4ms preprocess, 19.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 25.1ms\n","Speed: 5.4ms preprocess, 25.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 25.4ms\n","Speed: 2.6ms preprocess, 25.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 18.1ms\n","Speed: 2.4ms preprocess, 18.1ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 21.0ms\n","Speed: 2.4ms preprocess, 21.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 30.5ms\n","Speed: 2.9ms preprocess, 30.5ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 24.4ms\n","Speed: 5.5ms preprocess, 24.4ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 18.6ms\n","Speed: 4.2ms preprocess, 18.6ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 19.9ms\n","Speed: 2.3ms preprocess, 19.9ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 20.5ms\n","Speed: 2.3ms preprocess, 20.5ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 18.4ms\n","Speed: 2.4ms preprocess, 18.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 21.1ms\n","Speed: 2.4ms preprocess, 21.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.3ms\n","Speed: 2.3ms preprocess, 12.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.6ms\n","Speed: 2.3ms preprocess, 15.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.0ms\n","Speed: 2.3ms preprocess, 13.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.1ms\n","Speed: 3.6ms preprocess, 12.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.5ms\n","Speed: 2.3ms preprocess, 12.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.4ms\n","Speed: 2.2ms preprocess, 13.4ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.0ms\n","Speed: 2.2ms preprocess, 12.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.0ms\n","Speed: 2.4ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.0ms\n","Speed: 2.2ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 10.3ms\n","Speed: 2.5ms preprocess, 10.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.8ms\n","Speed: 2.3ms preprocess, 12.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.7ms\n","Speed: 2.3ms preprocess, 13.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.5ms\n","Speed: 2.3ms preprocess, 11.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.6ms\n","Speed: 2.2ms preprocess, 15.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 17.3ms\n","Speed: 3.3ms preprocess, 17.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.5ms\n","Speed: 2.3ms preprocess, 14.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.0ms\n","Speed: 2.3ms preprocess, 14.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.3ms\n","Speed: 3.2ms preprocess, 14.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 16.7ms\n","Speed: 3.7ms preprocess, 16.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.8ms\n","Speed: 2.2ms preprocess, 12.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.0ms\n","Speed: 2.3ms preprocess, 12.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.8ms\n","Speed: 2.3ms preprocess, 14.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.1ms\n","Speed: 2.4ms preprocess, 11.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.9ms\n","Speed: 2.2ms preprocess, 12.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.0ms\n","Speed: 2.3ms preprocess, 12.0ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.8ms\n","Speed: 2.7ms preprocess, 14.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.6ms\n","Speed: 2.4ms preprocess, 11.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.4ms\n","Speed: 2.3ms preprocess, 14.4ms inference, 6.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.9ms\n","Speed: 3.7ms preprocess, 11.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.1ms\n","Speed: 2.3ms preprocess, 13.1ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.3ms\n","Speed: 2.3ms preprocess, 12.3ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.9ms\n","Speed: 2.2ms preprocess, 13.9ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.5ms\n","Speed: 2.3ms preprocess, 11.5ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.8ms\n","Speed: 2.4ms preprocess, 11.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.5ms\n","Speed: 2.2ms preprocess, 12.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.0ms\n","Speed: 2.3ms preprocess, 14.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.5ms\n","Speed: 2.3ms preprocess, 11.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 10.9ms\n","Speed: 2.2ms preprocess, 10.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.7ms\n","Speed: 2.2ms preprocess, 11.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.9ms\n","Speed: 2.2ms preprocess, 12.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.9ms\n","Speed: 2.5ms preprocess, 14.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.5ms\n","Speed: 3.2ms preprocess, 14.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.8ms\n","Speed: 2.3ms preprocess, 15.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 18.8ms\n","Speed: 2.2ms preprocess, 18.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.3ms\n","Speed: 2.3ms preprocess, 15.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.9ms\n","Speed: 2.2ms preprocess, 11.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.5ms\n","Speed: 2.8ms preprocess, 12.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.4ms\n","Speed: 2.2ms preprocess, 11.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.4ms\n","Speed: 2.2ms preprocess, 14.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.6ms\n","Speed: 2.2ms preprocess, 13.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.5ms\n","Speed: 2.3ms preprocess, 12.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.9ms\n","Speed: 2.2ms preprocess, 12.9ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.9ms\n","Speed: 2.2ms preprocess, 12.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.9ms\n","Speed: 2.2ms preprocess, 14.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.0ms\n","Speed: 2.3ms preprocess, 12.0ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 16.3ms\n","Speed: 2.2ms preprocess, 16.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 20.3ms\n","Speed: 2.3ms preprocess, 20.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.8ms\n","Speed: 2.6ms preprocess, 13.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.0ms\n","Speed: 2.3ms preprocess, 12.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.1ms\n","Speed: 2.3ms preprocess, 12.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.4ms\n","Speed: 2.3ms preprocess, 15.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.4ms\n","Speed: 2.2ms preprocess, 12.4ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.1ms\n","Speed: 2.2ms preprocess, 11.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 10.6ms\n","Speed: 2.3ms preprocess, 10.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.0ms\n","Speed: 2.3ms preprocess, 13.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.0ms\n","Speed: 2.3ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.3ms\n","Speed: 2.3ms preprocess, 13.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.6ms\n","Speed: 2.2ms preprocess, 12.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.3ms\n","Speed: 5.5ms preprocess, 15.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.5ms\n","Speed: 2.3ms preprocess, 15.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 18.5ms\n","Speed: 3.5ms preprocess, 18.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 18.7ms\n","Speed: 2.2ms preprocess, 18.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 10.5ms\n","Speed: 3.0ms preprocess, 10.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.1ms\n","Speed: 2.2ms preprocess, 15.1ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.4ms\n","Speed: 2.2ms preprocess, 12.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 10.8ms\n","Speed: 2.3ms preprocess, 10.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 10.3ms\n","Speed: 2.2ms preprocess, 10.3ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.8ms\n","Speed: 2.2ms preprocess, 13.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.8ms\n","Speed: 2.3ms preprocess, 12.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.8ms\n","Speed: 2.2ms preprocess, 11.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.8ms\n","Speed: 2.3ms preprocess, 14.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.4ms\n","Speed: 2.2ms preprocess, 14.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.3ms\n","Speed: 2.3ms preprocess, 12.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.6ms\n","Speed: 2.2ms preprocess, 11.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 16.0ms\n","Speed: 2.2ms preprocess, 16.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.3ms\n","Speed: 2.2ms preprocess, 13.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.0ms\n","Speed: 2.2ms preprocess, 12.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.3ms\n","Speed: 2.2ms preprocess, 12.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 16.4ms\n","Speed: 2.2ms preprocess, 16.4ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.3ms\n","Speed: 2.3ms preprocess, 15.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.3ms\n","Speed: 2.2ms preprocess, 14.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.8ms\n","Speed: 2.2ms preprocess, 14.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.1ms\n","Speed: 2.2ms preprocess, 13.1ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.6ms\n","Speed: 4.1ms preprocess, 14.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.4ms\n","Speed: 2.2ms preprocess, 13.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.1ms\n","Speed: 2.2ms preprocess, 14.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.8ms\n","Speed: 2.2ms preprocess, 15.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.3ms\n","Speed: 2.6ms preprocess, 14.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 21.0ms\n","Speed: 2.2ms preprocess, 21.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 19.6ms\n","Speed: 3.1ms preprocess, 19.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 16.2ms\n","Speed: 2.4ms preprocess, 16.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.1ms\n","Speed: 2.3ms preprocess, 14.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.4ms\n","Speed: 2.3ms preprocess, 13.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.8ms\n","Speed: 2.2ms preprocess, 15.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.2ms\n","Speed: 4.2ms preprocess, 15.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 10.0ms\n","Speed: 2.2ms preprocess, 10.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.8ms\n","Speed: 3.0ms preprocess, 12.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.7ms\n","Speed: 2.2ms preprocess, 12.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.5ms\n","Speed: 2.6ms preprocess, 11.5ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 10.2ms\n","Speed: 2.3ms preprocess, 10.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 17.3ms\n","Speed: 2.2ms preprocess, 17.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.6ms\n","Speed: 2.9ms preprocess, 12.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 21.3ms\n","Speed: 2.2ms preprocess, 21.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.2ms\n","Speed: 2.4ms preprocess, 12.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.6ms\n","Speed: 2.2ms preprocess, 13.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.6ms\n","Speed: 3.2ms preprocess, 12.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.1ms\n","Speed: 2.4ms preprocess, 11.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.4ms\n","Speed: 2.3ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.2ms\n","Speed: 2.2ms preprocess, 13.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.7ms\n","Speed: 2.2ms preprocess, 12.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.7ms\n","Speed: 2.3ms preprocess, 13.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.6ms\n","Speed: 2.3ms preprocess, 12.6ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 17.4ms\n","Speed: 2.5ms preprocess, 17.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 16.0ms\n","Speed: 2.3ms preprocess, 16.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.6ms\n","Speed: 2.6ms preprocess, 15.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.4ms\n","Speed: 2.3ms preprocess, 15.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 19.3ms\n","Speed: 4.2ms preprocess, 19.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.0ms\n","Speed: 2.5ms preprocess, 11.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.9ms\n","Speed: 2.3ms preprocess, 11.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 10.3ms\n","Speed: 2.3ms preprocess, 10.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 16.0ms\n","Speed: 2.3ms preprocess, 16.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.1ms\n","Speed: 2.3ms preprocess, 12.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.0ms\n","Speed: 2.3ms preprocess, 13.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 10.2ms\n","Speed: 2.2ms preprocess, 10.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.2ms\n","Speed: 2.3ms preprocess, 14.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.8ms\n","Speed: 2.2ms preprocess, 11.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.0ms\n","Speed: 2.3ms preprocess, 13.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 10.8ms\n","Speed: 6.5ms preprocess, 10.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.2ms\n","Speed: 2.2ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 9.1ms\n","Speed: 2.3ms preprocess, 9.1ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 18.3ms\n","Speed: 2.2ms preprocess, 18.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.0ms\n","Speed: 2.6ms preprocess, 12.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 10.4ms\n","Speed: 2.3ms preprocess, 10.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.2ms\n","Speed: 2.2ms preprocess, 13.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 16.9ms\n","Speed: 2.3ms preprocess, 16.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.2ms\n","Speed: 2.3ms preprocess, 15.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.1ms\n","Speed: 2.3ms preprocess, 12.1ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 10.8ms\n","Speed: 2.2ms preprocess, 10.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.1ms\n","Speed: 2.3ms preprocess, 14.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 17.3ms\n","Speed: 3.8ms preprocess, 17.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 19.7ms\n","Speed: 4.3ms preprocess, 19.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 18.7ms\n","Speed: 3.7ms preprocess, 18.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 17.2ms\n","Speed: 5.2ms preprocess, 17.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 10.9ms\n","Speed: 2.3ms preprocess, 10.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.1ms\n","Speed: 2.3ms preprocess, 13.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 10.0ms\n","Speed: 2.3ms preprocess, 10.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.6ms\n","Speed: 2.2ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.2ms\n","Speed: 2.3ms preprocess, 14.2ms inference, 4.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.7ms\n","Speed: 2.4ms preprocess, 12.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.9ms\n","Speed: 2.3ms preprocess, 11.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.6ms\n","Speed: 2.3ms preprocess, 12.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.2ms\n","Speed: 2.3ms preprocess, 13.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.2ms\n","Speed: 2.6ms preprocess, 11.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.5ms\n","Speed: 2.2ms preprocess, 13.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.3ms\n","Speed: 2.2ms preprocess, 11.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.4ms\n","Speed: 2.3ms preprocess, 13.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.2ms\n","Speed: 2.3ms preprocess, 11.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.5ms\n","Speed: 2.2ms preprocess, 12.5ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.9ms\n","Speed: 2.2ms preprocess, 14.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.9ms\n","Speed: 2.2ms preprocess, 13.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.1ms\n","Speed: 2.4ms preprocess, 12.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 9.6ms\n","Speed: 2.3ms preprocess, 9.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.4ms\n","Speed: 2.2ms preprocess, 12.4ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.6ms\n","Speed: 2.2ms preprocess, 12.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 9.9ms\n","Speed: 2.7ms preprocess, 9.9ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 9.4ms\n","Speed: 2.3ms preprocess, 9.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.9ms\n","Speed: 2.3ms preprocess, 13.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 20.3ms\n","Speed: 2.4ms preprocess, 20.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 18.6ms\n","Speed: 2.9ms preprocess, 18.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.9ms\n","Speed: 2.7ms preprocess, 14.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 18.7ms\n","Speed: 2.3ms preprocess, 18.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.5ms\n","Speed: 2.4ms preprocess, 12.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.5ms\n","Speed: 2.2ms preprocess, 11.5ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.9ms\n","Speed: 2.3ms preprocess, 11.9ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 10.9ms\n","Speed: 2.2ms preprocess, 10.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.2ms\n","Speed: 2.3ms preprocess, 15.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.5ms\n","Speed: 2.2ms preprocess, 12.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.0ms\n","Speed: 5.4ms preprocess, 14.0ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.4ms\n","Speed: 2.3ms preprocess, 13.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 9.8ms\n","Speed: 2.5ms preprocess, 9.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.5ms\n","Speed: 2.2ms preprocess, 12.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.2ms\n","Speed: 5.4ms preprocess, 11.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.5ms\n","Speed: 2.3ms preprocess, 12.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 10.6ms\n","Speed: 2.2ms preprocess, 10.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.3ms\n","Speed: 2.2ms preprocess, 13.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.8ms\n","Speed: 2.2ms preprocess, 13.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.2ms\n","Speed: 2.4ms preprocess, 12.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.9ms\n","Speed: 2.3ms preprocess, 13.9ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.0ms\n","Speed: 2.3ms preprocess, 15.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.5ms\n","Speed: 2.3ms preprocess, 15.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.2ms\n","Speed: 2.3ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.0ms\n","Speed: 2.3ms preprocess, 15.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.2ms\n","Speed: 2.3ms preprocess, 13.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.4ms\n","Speed: 2.2ms preprocess, 13.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.8ms\n","Speed: 2.3ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 16.0ms\n","Speed: 2.3ms preprocess, 16.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 17.3ms\n","Speed: 2.3ms preprocess, 17.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 20.4ms\n","Speed: 2.2ms preprocess, 20.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 21.0ms\n","Speed: 2.5ms preprocess, 21.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 17.2ms\n","Speed: 4.5ms preprocess, 17.2ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 17.8ms\n","Speed: 2.3ms preprocess, 17.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.8ms\n","Speed: 2.2ms preprocess, 15.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 10.8ms\n","Speed: 2.4ms preprocess, 10.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 10.1ms\n","Speed: 2.3ms preprocess, 10.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.2ms\n","Speed: 2.2ms preprocess, 14.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.9ms\n","Speed: 2.4ms preprocess, 12.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.9ms\n","Speed: 2.3ms preprocess, 13.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.8ms\n","Speed: 2.4ms preprocess, 12.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 9.7ms\n","Speed: 2.2ms preprocess, 9.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.9ms\n","Speed: 2.3ms preprocess, 14.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.9ms\n","Speed: 2.2ms preprocess, 12.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 10.9ms\n","Speed: 2.2ms preprocess, 10.9ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 17.9ms\n","Speed: 2.2ms preprocess, 17.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.1ms\n","Speed: 2.2ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.4ms\n","Speed: 2.3ms preprocess, 11.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.4ms\n","Speed: 3.2ms preprocess, 11.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 18.2ms\n","Speed: 2.8ms preprocess, 18.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 9.0ms\n","Speed: 11.3ms preprocess, 9.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.0ms\n","Speed: 2.4ms preprocess, 13.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 10.0ms\n","Speed: 2.4ms preprocess, 10.0ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.5ms\n","Speed: 2.3ms preprocess, 15.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 22.2ms\n","Speed: 2.7ms preprocess, 22.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.8ms\n","Speed: 2.6ms preprocess, 15.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.6ms\n","Speed: 2.8ms preprocess, 13.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.7ms\n","Speed: 2.3ms preprocess, 15.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 16.3ms\n","Speed: 3.7ms preprocess, 16.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.8ms\n","Speed: 2.3ms preprocess, 12.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.5ms\n","Speed: 2.3ms preprocess, 12.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.6ms\n","Speed: 2.2ms preprocess, 12.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.6ms\n","Speed: 2.2ms preprocess, 13.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.5ms\n","Speed: 2.4ms preprocess, 14.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.9ms\n","Speed: 2.2ms preprocess, 12.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.1ms\n","Speed: 2.4ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.6ms\n","Speed: 2.4ms preprocess, 13.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 10.5ms\n","Speed: 2.4ms preprocess, 10.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.5ms\n","Speed: 2.2ms preprocess, 15.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.4ms\n","Speed: 2.4ms preprocess, 14.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.6ms\n","Speed: 2.2ms preprocess, 12.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.6ms\n","Speed: 2.7ms preprocess, 12.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.0ms\n","Speed: 2.4ms preprocess, 13.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.9ms\n","Speed: 2.3ms preprocess, 12.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.4ms\n","Speed: 2.2ms preprocess, 14.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.2ms\n","Speed: 2.2ms preprocess, 15.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 10.3ms\n","Speed: 2.5ms preprocess, 10.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 19.7ms\n","Speed: 3.8ms preprocess, 19.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.6ms\n","Speed: 2.5ms preprocess, 13.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.0ms\n","Speed: 2.3ms preprocess, 13.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.5ms\n","Speed: 2.3ms preprocess, 15.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 17.6ms\n","Speed: 3.6ms preprocess, 17.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 18.4ms\n","Speed: 2.4ms preprocess, 18.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.4ms\n","Speed: 3.6ms preprocess, 14.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 19.8ms\n","Speed: 3.5ms preprocess, 19.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.4ms\n","Speed: 2.8ms preprocess, 13.4ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.5ms\n","Speed: 2.2ms preprocess, 14.5ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.6ms\n","Speed: 2.2ms preprocess, 13.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.7ms\n","Speed: 2.2ms preprocess, 12.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.8ms\n","Speed: 2.3ms preprocess, 12.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.5ms\n","Speed: 2.3ms preprocess, 12.5ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 16.2ms\n","Speed: 3.1ms preprocess, 16.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.8ms\n","Speed: 2.2ms preprocess, 12.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 10.0ms\n","Speed: 2.2ms preprocess, 10.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 16.8ms\n","Speed: 2.2ms preprocess, 16.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 21.5ms\n","Speed: 2.3ms preprocess, 21.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 23.8ms\n","Speed: 2.6ms preprocess, 23.8ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 18.2ms\n","Speed: 3.9ms preprocess, 18.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.2ms\n","Speed: 3.0ms preprocess, 15.2ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 22.5ms\n","Speed: 6.5ms preprocess, 22.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.7ms\n","Speed: 5.2ms preprocess, 15.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.3ms\n","Speed: 2.3ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 16.2ms\n","Speed: 2.2ms preprocess, 16.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 17.9ms\n","Speed: 2.3ms preprocess, 17.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 21.2ms\n","Speed: 2.2ms preprocess, 21.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.7ms\n","Speed: 3.1ms preprocess, 15.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 18.5ms\n","Speed: 2.3ms preprocess, 18.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 20.5ms\n","Speed: 2.2ms preprocess, 20.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 21.4ms\n","Speed: 2.2ms preprocess, 21.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.9ms\n","Speed: 2.2ms preprocess, 15.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.6ms\n","Speed: 4.0ms preprocess, 11.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 18.2ms\n","Speed: 2.3ms preprocess, 18.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 36.0ms\n","Speed: 2.3ms preprocess, 36.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 16.5ms\n","Speed: 2.4ms preprocess, 16.5ms inference, 5.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.2ms\n","Speed: 2.7ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 30.5ms\n","Speed: 2.4ms preprocess, 30.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 16.0ms\n","Speed: 2.4ms preprocess, 16.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.6ms\n","Speed: 3.0ms preprocess, 11.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.2ms\n","Speed: 2.2ms preprocess, 13.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.9ms\n","Speed: 3.0ms preprocess, 11.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 17.8ms\n","Speed: 2.3ms preprocess, 17.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.1ms\n","Speed: 3.0ms preprocess, 13.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.9ms\n","Speed: 2.4ms preprocess, 11.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.8ms\n","Speed: 3.2ms preprocess, 12.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 19.7ms\n","Speed: 2.4ms preprocess, 19.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 19.6ms\n","Speed: 2.3ms preprocess, 19.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 17.3ms\n","Speed: 2.4ms preprocess, 17.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.8ms\n","Speed: 5.4ms preprocess, 12.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 19.6ms\n","Speed: 2.3ms preprocess, 19.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 17.2ms\n","Speed: 2.3ms preprocess, 17.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 21.5ms\n","Speed: 2.2ms preprocess, 21.5ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 17.0ms\n","Speed: 2.3ms preprocess, 17.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 16.7ms\n","Speed: 2.2ms preprocess, 16.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.4ms\n","Speed: 2.5ms preprocess, 14.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 17.2ms\n","Speed: 2.2ms preprocess, 17.2ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 19.8ms\n","Speed: 2.3ms preprocess, 19.8ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 22.0ms\n","Speed: 2.3ms preprocess, 22.0ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.6ms\n","Speed: 2.2ms preprocess, 15.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.6ms\n","Speed: 2.3ms preprocess, 14.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 22.6ms\n","Speed: 2.3ms preprocess, 22.6ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.4ms\n","Speed: 2.2ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 16.7ms\n","Speed: 2.3ms preprocess, 16.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 20.5ms\n","Speed: 2.3ms preprocess, 20.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.8ms\n","Speed: 5.9ms preprocess, 13.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.0ms\n","Speed: 2.2ms preprocess, 13.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.0ms\n","Speed: 2.5ms preprocess, 14.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 16.5ms\n","Speed: 2.2ms preprocess, 16.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.7ms\n","Speed: 2.2ms preprocess, 15.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 20.0ms\n","Speed: 3.5ms preprocess, 20.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.5ms\n","Speed: 2.3ms preprocess, 14.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.2ms\n","Speed: 2.2ms preprocess, 12.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.2ms\n","Speed: 2.2ms preprocess, 12.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 16.5ms\n","Speed: 2.2ms preprocess, 16.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.3ms\n","Speed: 2.6ms preprocess, 12.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 16.3ms\n","Speed: 2.1ms preprocess, 16.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 20.6ms\n","Speed: 2.3ms preprocess, 20.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 16.8ms\n","Speed: 2.3ms preprocess, 16.8ms inference, 4.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 18.8ms\n","Speed: 2.2ms preprocess, 18.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.3ms\n","Speed: 2.2ms preprocess, 14.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.2ms\n","Speed: 2.1ms preprocess, 13.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.0ms\n","Speed: 2.2ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.5ms\n","Speed: 2.2ms preprocess, 14.5ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 17.3ms\n","Speed: 2.2ms preprocess, 17.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.7ms\n","Speed: 2.1ms preprocess, 12.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.3ms\n","Speed: 2.2ms preprocess, 13.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.2ms\n","Speed: 2.2ms preprocess, 13.2ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.4ms\n","Speed: 2.2ms preprocess, 15.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 18.8ms\n","Speed: 2.3ms preprocess, 18.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.8ms\n","Speed: 3.2ms preprocess, 15.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.4ms\n","Speed: 3.5ms preprocess, 15.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.7ms\n","Speed: 2.9ms preprocess, 15.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.1ms\n","Speed: 2.2ms preprocess, 14.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 21.6ms\n","Speed: 2.6ms preprocess, 21.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 17.2ms\n","Speed: 9.6ms preprocess, 17.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 23.0ms\n","Speed: 2.2ms preprocess, 23.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.0ms\n","Speed: 2.2ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 18.7ms\n","Speed: 2.2ms preprocess, 18.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 16.9ms\n","Speed: 2.2ms preprocess, 16.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.7ms\n","Speed: 2.2ms preprocess, 12.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 16.7ms\n","Speed: 2.3ms preprocess, 16.7ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.0ms\n","Speed: 2.3ms preprocess, 11.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 17.4ms\n","Speed: 2.2ms preprocess, 17.4ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.7ms\n","Speed: 5.3ms preprocess, 14.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 18.9ms\n","Speed: 2.2ms preprocess, 18.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.4ms\n","Speed: 2.2ms preprocess, 15.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.6ms\n","Speed: 2.2ms preprocess, 11.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 16.4ms\n","Speed: 2.4ms preprocess, 16.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 17.4ms\n","Speed: 2.8ms preprocess, 17.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 19.7ms\n","Speed: 2.2ms preprocess, 19.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.8ms\n","Speed: 2.3ms preprocess, 12.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 20.1ms\n","Speed: 2.3ms preprocess, 20.1ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 18.6ms\n","Speed: 3.5ms preprocess, 18.6ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 23.9ms\n","Speed: 4.6ms preprocess, 23.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.9ms\n","Speed: 3.8ms preprocess, 15.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.1ms\n","Speed: 3.5ms preprocess, 12.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.4ms\n","Speed: 2.3ms preprocess, 14.4ms inference, 5.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 19.1ms\n","Speed: 2.3ms preprocess, 19.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.7ms\n","Speed: 3.1ms preprocess, 12.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 21.5ms\n","Speed: 3.4ms preprocess, 21.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 21.0ms\n","Speed: 2.3ms preprocess, 21.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 16.6ms\n","Speed: 2.2ms preprocess, 16.6ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 18.7ms\n","Speed: 2.2ms preprocess, 18.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 17.3ms\n","Speed: 2.2ms preprocess, 17.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 17.1ms\n","Speed: 2.5ms preprocess, 17.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 20.0ms\n","Speed: 2.9ms preprocess, 20.0ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 20.3ms\n","Speed: 2.3ms preprocess, 20.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 20.9ms\n","Speed: 2.0ms preprocess, 20.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 19.2ms\n","Speed: 2.3ms preprocess, 19.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.0ms\n","Speed: 2.6ms preprocess, 14.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 19.1ms\n","Speed: 2.2ms preprocess, 19.1ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.6ms\n","Speed: 2.3ms preprocess, 15.6ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 20.2ms\n","Speed: 2.3ms preprocess, 20.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 24.6ms\n","Speed: 5.1ms preprocess, 24.6ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 20.2ms\n","Speed: 3.5ms preprocess, 20.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 16.8ms\n","Speed: 2.3ms preprocess, 16.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.1ms\n","Speed: 2.3ms preprocess, 11.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 16.0ms\n","Speed: 2.2ms preprocess, 16.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.8ms\n","Speed: 2.3ms preprocess, 13.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.8ms\n","Speed: 2.7ms preprocess, 15.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.8ms\n","Speed: 2.2ms preprocess, 12.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.5ms\n","Speed: 2.2ms preprocess, 12.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.5ms\n","Speed: 2.3ms preprocess, 12.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.8ms\n","Speed: 3.4ms preprocess, 13.8ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.2ms\n","Speed: 2.4ms preprocess, 13.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.6ms\n","Speed: 4.1ms preprocess, 11.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.1ms\n","Speed: 2.3ms preprocess, 13.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.1ms\n","Speed: 3.6ms preprocess, 11.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.4ms\n","Speed: 2.3ms preprocess, 12.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.0ms\n","Speed: 2.3ms preprocess, 12.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.4ms\n","Speed: 2.3ms preprocess, 11.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.3ms\n","Speed: 2.5ms preprocess, 13.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 9.6ms\n","Speed: 4.4ms preprocess, 9.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.2ms\n","Speed: 2.3ms preprocess, 14.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.5ms\n","Speed: 2.2ms preprocess, 12.5ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.3ms\n","Speed: 2.3ms preprocess, 12.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.8ms\n","Speed: 2.3ms preprocess, 12.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.9ms\n","Speed: 2.3ms preprocess, 13.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.9ms\n","Speed: 2.3ms preprocess, 15.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.3ms\n","Speed: 4.4ms preprocess, 15.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 18.4ms\n","Speed: 3.8ms preprocess, 18.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.7ms\n","Speed: 2.3ms preprocess, 15.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 16.5ms\n","Speed: 2.3ms preprocess, 16.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.8ms\n","Speed: 2.3ms preprocess, 15.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.2ms\n","Speed: 2.3ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.8ms\n","Speed: 2.2ms preprocess, 14.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 16.7ms\n","Speed: 2.3ms preprocess, 16.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.9ms\n","Speed: 2.4ms preprocess, 15.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.7ms\n","Speed: 2.3ms preprocess, 12.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.5ms\n","Speed: 2.4ms preprocess, 11.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.7ms\n","Speed: 2.3ms preprocess, 13.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.6ms\n","Speed: 2.3ms preprocess, 13.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.8ms\n","Speed: 2.3ms preprocess, 13.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.2ms\n","Speed: 2.4ms preprocess, 13.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.6ms\n","Speed: 2.3ms preprocess, 13.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.2ms\n","Speed: 3.5ms preprocess, 14.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.0ms\n","Speed: 2.3ms preprocess, 12.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.6ms\n","Speed: 2.3ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.4ms\n","Speed: 2.2ms preprocess, 14.4ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.0ms\n","Speed: 2.2ms preprocess, 12.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 10.5ms\n","Speed: 4.7ms preprocess, 10.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.2ms\n","Speed: 2.2ms preprocess, 13.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.8ms\n","Speed: 2.2ms preprocess, 13.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.7ms\n","Speed: 2.3ms preprocess, 14.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.0ms\n","Speed: 2.3ms preprocess, 12.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 19.4ms\n","Speed: 3.5ms preprocess, 19.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.5ms\n","Speed: 2.4ms preprocess, 15.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.2ms\n","Speed: 2.5ms preprocess, 15.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 16.0ms\n","Speed: 2.9ms preprocess, 16.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.6ms\n","Speed: 2.3ms preprocess, 12.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.6ms\n","Speed: 2.2ms preprocess, 11.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.3ms\n","Speed: 2.3ms preprocess, 12.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.8ms\n","Speed: 2.2ms preprocess, 12.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 17.2ms\n","Speed: 2.3ms preprocess, 17.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.8ms\n","Speed: 4.3ms preprocess, 12.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.7ms\n","Speed: 2.2ms preprocess, 15.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.4ms\n","Speed: 2.3ms preprocess, 12.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 10.7ms\n","Speed: 2.2ms preprocess, 10.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 9.6ms\n","Speed: 4.5ms preprocess, 9.6ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.4ms\n","Speed: 2.5ms preprocess, 14.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 24.3ms\n","Speed: 2.2ms preprocess, 24.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.3ms\n","Speed: 2.3ms preprocess, 15.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.8ms\n","Speed: 2.3ms preprocess, 13.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.0ms\n","Speed: 2.3ms preprocess, 13.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.0ms\n","Speed: 2.2ms preprocess, 14.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 10.5ms\n","Speed: 2.3ms preprocess, 10.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.6ms\n","Speed: 2.3ms preprocess, 13.6ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.1ms\n","Speed: 2.2ms preprocess, 14.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 17.8ms\n","Speed: 2.3ms preprocess, 17.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.4ms\n","Speed: 2.3ms preprocess, 15.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.2ms\n","Speed: 2.3ms preprocess, 13.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 18.8ms\n","Speed: 3.8ms preprocess, 18.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.5ms\n","Speed: 2.4ms preprocess, 14.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 18.4ms\n","Speed: 6.1ms preprocess, 18.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 16.1ms\n","Speed: 2.2ms preprocess, 16.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.9ms\n","Speed: 2.4ms preprocess, 15.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.5ms\n","Speed: 2.3ms preprocess, 11.5ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.8ms\n","Speed: 2.3ms preprocess, 12.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.7ms\n","Speed: 2.2ms preprocess, 12.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 10.7ms\n","Speed: 2.6ms preprocess, 10.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.8ms\n","Speed: 2.3ms preprocess, 13.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 10.3ms\n","Speed: 2.2ms preprocess, 10.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.0ms\n","Speed: 2.2ms preprocess, 13.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.4ms\n","Speed: 2.7ms preprocess, 14.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 10.8ms\n","Speed: 2.2ms preprocess, 10.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.5ms\n","Speed: 2.3ms preprocess, 12.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.5ms\n","Speed: 2.2ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.9ms\n","Speed: 2.4ms preprocess, 11.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.0ms\n","Speed: 5.3ms preprocess, 12.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.6ms\n","Speed: 2.2ms preprocess, 12.6ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.3ms\n","Speed: 2.2ms preprocess, 12.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.1ms\n","Speed: 2.2ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.4ms\n","Speed: 2.2ms preprocess, 14.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.1ms\n","Speed: 2.3ms preprocess, 12.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.6ms\n","Speed: 2.3ms preprocess, 12.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.5ms\n","Speed: 2.3ms preprocess, 11.5ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.6ms\n","Speed: 2.3ms preprocess, 12.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.5ms\n","Speed: 2.4ms preprocess, 12.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.3ms\n","Speed: 2.4ms preprocess, 12.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 18.1ms\n","Speed: 3.7ms preprocess, 18.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 18.5ms\n","Speed: 2.3ms preprocess, 18.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 16.7ms\n","Speed: 2.3ms preprocess, 16.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 16.2ms\n","Speed: 2.3ms preprocess, 16.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.5ms\n","Speed: 2.3ms preprocess, 12.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.0ms\n","Speed: 2.3ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.7ms\n","Speed: 2.3ms preprocess, 12.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 10.1ms\n","Speed: 2.2ms preprocess, 10.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.2ms\n","Speed: 2.2ms preprocess, 11.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.3ms\n","Speed: 2.3ms preprocess, 13.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.8ms\n","Speed: 2.3ms preprocess, 12.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.3ms\n","Speed: 3.9ms preprocess, 11.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.0ms\n","Speed: 2.2ms preprocess, 14.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.5ms\n","Speed: 2.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.4ms\n","Speed: 2.3ms preprocess, 14.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.8ms\n","Speed: 2.2ms preprocess, 11.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.3ms\n","Speed: 2.9ms preprocess, 11.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.0ms\n","Speed: 2.3ms preprocess, 11.0ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.2ms\n","Speed: 2.2ms preprocess, 12.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.9ms\n","Speed: 2.2ms preprocess, 13.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.3ms\n","Speed: 2.2ms preprocess, 14.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.7ms\n","Speed: 2.2ms preprocess, 12.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.3ms\n","Speed: 2.3ms preprocess, 12.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.1ms\n","Speed: 2.2ms preprocess, 13.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.9ms\n","Speed: 2.4ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.1ms\n","Speed: 2.2ms preprocess, 13.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.0ms\n","Speed: 2.2ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.2ms\n","Speed: 6.3ms preprocess, 12.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.9ms\n","Speed: 2.4ms preprocess, 12.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 16.8ms\n","Speed: 2.3ms preprocess, 16.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 17.5ms\n","Speed: 2.3ms preprocess, 17.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 16.2ms\n","Speed: 2.3ms preprocess, 16.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.1ms\n","Speed: 3.0ms preprocess, 15.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 9.2ms\n","Speed: 1.8ms preprocess, 9.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.3ms\n","Speed: 2.2ms preprocess, 12.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.0ms\n","Speed: 2.2ms preprocess, 13.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.5ms\n","Speed: 2.3ms preprocess, 13.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.1ms\n","Speed: 2.3ms preprocess, 12.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.5ms\n","Speed: 4.2ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.3ms\n","Speed: 2.2ms preprocess, 12.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 10.1ms\n","Speed: 2.4ms preprocess, 10.1ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.1ms\n","Speed: 2.2ms preprocess, 13.1ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.5ms\n","Speed: 2.3ms preprocess, 12.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 17.7ms\n","Speed: 2.3ms preprocess, 17.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.2ms\n","Speed: 2.3ms preprocess, 13.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 10.6ms\n","Speed: 2.2ms preprocess, 10.6ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.8ms\n","Speed: 2.2ms preprocess, 12.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.8ms\n","Speed: 2.2ms preprocess, 13.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.8ms\n","Speed: 2.4ms preprocess, 12.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.2ms\n","Speed: 2.3ms preprocess, 11.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.5ms\n","Speed: 3.3ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.8ms\n","Speed: 2.2ms preprocess, 14.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 10.1ms\n","Speed: 2.3ms preprocess, 10.1ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.5ms\n","Speed: 2.2ms preprocess, 13.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.9ms\n","Speed: 2.4ms preprocess, 11.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 16.3ms\n","Speed: 2.4ms preprocess, 16.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 17.1ms\n","Speed: 2.3ms preprocess, 17.1ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 17.7ms\n","Speed: 3.4ms preprocess, 17.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 19.7ms\n","Speed: 2.5ms preprocess, 19.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.8ms\n","Speed: 2.3ms preprocess, 14.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.8ms\n","Speed: 2.4ms preprocess, 12.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.6ms\n","Speed: 2.2ms preprocess, 13.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.0ms\n","Speed: 2.2ms preprocess, 12.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.3ms\n","Speed: 2.4ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.6ms\n","Speed: 2.2ms preprocess, 12.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.1ms\n","Speed: 2.3ms preprocess, 13.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.8ms\n","Speed: 2.2ms preprocess, 11.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.2ms\n","Speed: 2.2ms preprocess, 13.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.8ms\n","Speed: 2.5ms preprocess, 15.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.0ms\n","Speed: 2.3ms preprocess, 11.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.2ms\n","Speed: 2.3ms preprocess, 15.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.9ms\n","Speed: 2.3ms preprocess, 12.9ms inference, 5.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.5ms\n","Speed: 2.5ms preprocess, 12.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.8ms\n","Speed: 2.2ms preprocess, 11.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.5ms\n","Speed: 2.2ms preprocess, 12.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 10.1ms\n","Speed: 2.3ms preprocess, 10.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.0ms\n","Speed: 2.2ms preprocess, 13.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.9ms\n","Speed: 2.2ms preprocess, 11.9ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 10.5ms\n","Speed: 2.3ms preprocess, 10.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.8ms\n","Speed: 2.9ms preprocess, 11.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.5ms\n","Speed: 2.3ms preprocess, 13.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 16.2ms\n","Speed: 2.3ms preprocess, 16.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.8ms\n","Speed: 3.1ms preprocess, 14.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 16.2ms\n","Speed: 4.1ms preprocess, 16.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 16.5ms\n","Speed: 2.3ms preprocess, 16.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.9ms\n","Speed: 8.0ms preprocess, 14.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 10.7ms\n","Speed: 5.6ms preprocess, 10.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.1ms\n","Speed: 2.3ms preprocess, 12.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.0ms\n","Speed: 2.3ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.7ms\n","Speed: 2.4ms preprocess, 15.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.3ms\n","Speed: 2.3ms preprocess, 12.3ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.7ms\n","Speed: 2.2ms preprocess, 12.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.9ms\n","Speed: 2.2ms preprocess, 13.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.6ms\n","Speed: 2.2ms preprocess, 12.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.4ms\n","Speed: 2.3ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.5ms\n","Speed: 2.0ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.8ms\n","Speed: 2.2ms preprocess, 13.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.8ms\n","Speed: 2.6ms preprocess, 12.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.5ms\n","Speed: 2.4ms preprocess, 12.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.2ms\n","Speed: 3.3ms preprocess, 15.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 10.4ms\n","Speed: 3.4ms preprocess, 10.4ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.4ms\n","Speed: 2.2ms preprocess, 11.4ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.8ms\n","Speed: 2.3ms preprocess, 15.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.9ms\n","Speed: 2.4ms preprocess, 12.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 10.5ms\n","Speed: 3.4ms preprocess, 10.5ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 10.5ms\n","Speed: 2.5ms preprocess, 10.5ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.3ms\n","Speed: 2.3ms preprocess, 13.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.3ms\n","Speed: 2.3ms preprocess, 12.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.4ms\n","Speed: 3.8ms preprocess, 14.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 20.2ms\n","Speed: 2.9ms preprocess, 20.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 19.6ms\n","Speed: 2.3ms preprocess, 19.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 16.4ms\n","Speed: 3.9ms preprocess, 16.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.4ms\n","Speed: 2.4ms preprocess, 13.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 14.4ms\n","Speed: 2.3ms preprocess, 14.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.9ms\n","Speed: 2.2ms preprocess, 11.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.1ms\n","Speed: 2.3ms preprocess, 11.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.6ms\n","Speed: 2.5ms preprocess, 11.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.8ms\n","Speed: 2.2ms preprocess, 13.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.7ms\n","Speed: 2.2ms preprocess, 13.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.1ms\n","Speed: 2.2ms preprocess, 12.1ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.2ms\n","Speed: 2.2ms preprocess, 13.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.6ms\n","Speed: 2.2ms preprocess, 13.6ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.1ms\n","Speed: 2.2ms preprocess, 13.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 10.6ms\n","Speed: 3.1ms preprocess, 10.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 10.9ms\n","Speed: 3.1ms preprocess, 10.9ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.1ms\n","Speed: 6.7ms preprocess, 11.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.3ms\n","Speed: 2.2ms preprocess, 13.3ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 19.4ms\n","Speed: 2.2ms preprocess, 19.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.1ms\n","Speed: 2.4ms preprocess, 11.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 15.2ms\n","Speed: 2.7ms preprocess, 15.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 13.3ms\n","Speed: 2.3ms preprocess, 13.3ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 12.2ms\n","Speed: 2.2ms preprocess, 12.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 11.0ms\n","Speed: 2.4ms preprocess, 11.0ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","Video processing complete!\n","MP4 file save! \n"]}]},{"cell_type":"code","source":["model_path = \"/content/drive/MyDrive/YOLO/yolov11n-finetuning/output/train/weights/best.pt\"\n","model = YOLO(model_path)\n","\n","# 2. ì…ë ¥ ì´ë¯¸ì§€ ë¡œë“œ\n","img_path = \"/content/drive/MyDrive/YOLO/test_input/fire1.jpg\"\n","img = cv2.imread(img_path)\n","\n","if img is None:\n","    print(f\"âŒ Error: Image file could not be loaded. Check the file path:\\n{img_path}\")\n","    exit()\n","\n","height, width, _ = img.shape\n","print(f\"âœ… Image Loaded: ({width}x{height})\")\n","\n","# 3. YOLO ê°ì²´ íƒì§€ ì‹¤í–‰\n","results = model(img)\n","\n","# 4. íƒì§€ ê²°ê³¼ê°€ ê·¸ë ¤ì§„ ì´ë¯¸ì§€ ì–»ê¸°\n","annotated_img = results[0].plot()\n","\n","# 5. ê²°ê³¼ ì´ë¯¸ì§€ë¥¼ íŒŒì¼ë¡œ ì €ì¥\n","save_path = \"runs/detect/predict1/test_output_fire1.jpg\"\n","os.makedirs(os.path.dirname(save_path), exist_ok=True)\n","cv2.imwrite(save_path, annotated_img)\n","\n","print(\"âœ… Inference complete! Annotated image saved:\", save_path)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3y5chUhP2Wu_","executionInfo":{"status":"ok","timestamp":1742010158208,"user_tz":-540,"elapsed":2306,"user":{"displayName":"ê¹€ì˜ˆí–¥","userId":"01564514346289315086"}},"outputId":"74a1b393-d31d-4ccd-80d4-5da805d7eea6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… Image Loaded: (640x640)\n","\n","0: 640x640 2 fires, 27.6ms\n","Speed: 4.6ms preprocess, 27.6ms inference, 261.5ms postprocess per image at shape (1, 3, 640, 640)\n","âœ… Inference complete! Annotated image saved: runs/detect/predict1/test_output_fire1.jpg\n"]}]},{"cell_type":"markdown","source":["### ëª¨ë¸ í•™ìŠµ 2"],"metadata":{"id":"dGRhG5wY4O3m"}},{"cell_type":"code","source":["!pip install wandb\n","!wandb login ad541938b9cdba0c8991d9905ec5456ed72acbb0"],"metadata":{"id":"de2N0_1D2c7v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from Detectify import Train\n","\n","train = Train()\n","\n","train.start(dataset=\"/content/drive/MyDrive/YOLO/yolov11n-finetuning/data.yaml\")"],"metadata":{"id":"AqLYgpBv4N-2"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"orig_nbformat":4,"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}